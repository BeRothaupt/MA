% !TeX root = ../../../Main.tex
\chapter{Discussion}

In this work, three dynamic programming algorithms have been implemented and used for glider trajectory optimization in a calm environment. Their results were compared to an optimal control result and it has been investigated, if pre-training with DP helps convergence of a TRPO algorithm. The three algorithms all converged to a near-optimal solution in a scenario where 500m were to cover and the initial state was known a priori. Value iteration took the least time and was therefore chosen for studies in scenarios with distances of 1000m and 2000m. Results showed that TRPO performance is improved by DP. But if the pre-training time had been used for more TRPO iterations instead, the total calculation time would have been even lower.

Based on the results in chapter~\ref{chapter6}, one could argue, that dynamic programming does not make sense in this context. With a sufficiently fine state grid, even value iteration takes eight times as long as the optimal control benchmark in scenario 1 and only yields a suboptimal result in all scenarios. If compared to TRPO, a policy optimization algorithm (cf.~\cite{Zuern2017}), it performs worse with respect to calculation time as well as flight time. It also cannot deal with unknown upwinds, which TRPO can.

A big advantage of DP is however, that it inherently covers the whole state space. If the result of one of the DP algorithms is used to train an artificial neural network, an agent utilizing it can get to the goal from every point in the state space\footnote{This is only true, if reaching the goal is physically possible at all. A glider can, for example, not reach a point that is 1000m away, if its altitude is insufficient and there are no upwinds.}. This is not true for TRPO and OC, which both optimize a specific trajectory from a specific start state to the goal. As a tool for policy initialization, DP is especially helpful, if the start state (and therefore the approximate trajectory) is not known a priori.

Even if the start state is known, DP has an impact on the results of TRPO. As the results in Fig. \ref{tikz:trpoglorotvi1000mMax} show, pre training with a DP algorithm increases the initial average and maximum return per iteration for a TRPO algorihm. If the stopping criterion of TRPO is reaching a specific average return (i.e. a specific average flight time), Fig. \ref{tikz:trpoglorotvi1000mAvg} shows that this goal can be reached earlier than with standard TRPO and a Glorot-initialized policy ANN. For example, a pre-trained policy net reaches an average return of $50$ after only 80 iterations, while a Glorot-initialized policy net takes about 130 iterations. Whether this higher return in early TRPO iterations justifies the additional expense of performing a DP pre-training, depends on the performance of the TRPO algorithm and can therefore not be answered generally. A DP result can also be calculated only once and then stored and used for multiple TRPO calculations afterwards (e.g. with random updrafts or horizontal winds). This way, the time DP takes carries less weight if compared to TRPO without pre-training.