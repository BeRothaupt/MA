% !TeX root = ../../../Main.tex
\chapter{Discussion}
\label{chapter7}
In this chapter, the results of chapter \ref{chapter6} are discussed briefly.

All algorithms are able to yield trajectories to the goal from various initial states. As for the performance of PI and VI, the latter has several advantages. It is easier to implement, because it takes less steps per update than PI (cf. section~\ref{sec:appendix_pi_vi}). This also makes it easier to optimize VI for a specific problem structure. In this case, performing VI layer by layer only takes minor changes to the code, whereas PI takes more adjustment. Because VI takes less steps for a state value update, each value iteration also takes less time in total (cf. appendix~\ref{appendix_B}).

In the first scenario, the DP algorithms perform as expected, directing the agent to the goal. Due to the coarse discretization, the flight times are significantly longer than the optimal flight time. Value iteration takes the least time and is therefore chosen to be used for further investigation. With a fine grid, VI yields a nearly optimal trajectory.

In scenario 2, only VI was investigated. The algorithm manages to find a trajectory to the goal, but is significantly worse than the optimal control this time - even on a fine grid. The trajectory and control sequences are similar to what is achieved by A3C in \cite{Zuern2017}. This suggests, that the use of value functions has something to do with the observed behavior. Nevertheless, the resulting policy can be used as a starting point for TRPO.

By modifying the PI and VI algorithms to work asynchronously, calculation times can be reduced significantly. The asynchronous realizations of both algorithms are faster than their synchronous counterparts by up to 72 \%. This confirms the remark from \cite{Powell2007ADP}, that modifying any DP algorithm to exploit the problem structure helps a lot when it comes to calculation time.

Another topic in section~\ref{sec:dp_init_trpo} is, whether policy optimization by TRPO distorts the outputs of a policy obtained by a DP algorithm. As can be seen in table~\ref{tab:vi_trpo_flighttimes}, this is the case to some extent. An agent with a pre-trained policy ($\pi_1$) still finds the goal after policy optimization through TRPO ($\pi_3$), if the start state is sufficiently close to the start state of TRPO. More concisely, all trajectories, that start at states between $x_0 = 0\text{m}$ and $x_0 = 300\text{m}$, are improved by TRPO after DP. In fact, the results are even better than the results obtained by only performing TRPO ($\pi_2$). Between $x_0 = 400\text{m}$ and $x_0 = 800\text{m}$, $\pi_1$ is the only policy that is able to direct the agent to the target reliably. 
As these results suggest, a combination of DP and TRPO can retain some of the advantages of both algorithms, making DP a viable method to enhance TRPO performance and results. All in all, a combination of $\pi_1$ and $\pi_3$ would probably make the most sense. At points, where $\pi_3$ fails, the agent can still fall back on $\pi_1$ and reach the goal.
