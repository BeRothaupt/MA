% !TeX root = ../../../Main.tex
\chapter{Results}
\label{chapter6}

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/opti500m-coarse.dat}}\dataOPTIfiveH

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/opti1000m-coarse.dat}}\dataOPTIoneK

%\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/opti2000m-coarse.dat}}\dataOPTItwoK

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no118-i3-500m-dt250ms_linear_table.dat}}\dataVIfiveHcoarse

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no115-i17-500m-dt250ms_linear_table.dat}}\dataOPIfiveHcoarse

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no119-i14-500m-dt250ms_linear_table.dat}}\dataGPIfiveHcoarse


\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no108-i7-500m-dt250ms.dat}}\dataVIfiveHfine

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no120-i0-1000m-dt250ms.dat}}\dataVIoneK

%\pgfplotstableread[col sep=comma] {\detokenize{}}\dataVItwoK

\pgfplotstableread[col sep=comma]
{\detokenize{./src/pics/tikz/trpo-glorot-vi-1km-AvgRet.dat}}\trpoGlorotVsVIoneKAvgReturn

\pgfplotstableread[col sep=comma]
{\detokenize{./src/pics/tikz/trpo-glorot-vi-1km-MaxRet.dat}}\trpoGlorotVsVIoneKMaxReturn

\section{2D Dynamic Programming}

\subsection{Optimal Control Benchmark}
\label{sec:results_dp}
In the 2D scenario, three Dynamic Programming algorithms are compared:

\begin{itemize}
	\item Generalized policy iteration (cf. section \ref{sec:PI})
	\item Optimistic policy iteration (cf. section \ref{sec:PI})
	\item Value iteration (cf. section \ref{subsection:VI})
\end{itemize}

The agent faces three different scenarios, each with calm air. Table \ref{tab:scenario_data} shows the distance to cover, start state, and the mean and standard deviation for state normalization (cf.~\ref{sec:input_standardization}) for each scenario. The agent starts at a height of 100m and with zero velocity in all scenarios. For each scenario, there is a separate mean $\mu$ and standard deviation $\sigma$ for each coordinate. Therefore, each column contains a vector of means and standard deviations.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|c c c}
			scenario & 1 & 2 & 3 \\ \hline
			distance $d_T$ & 500m & 1000m & 2000m\\
			start state $s_0$ & $s_0 = [0, 100, 0, 0]^\top$ &  $s_0 = [0, 100, 0, 0]^\top$  &  $s_0 = [0, 100, 0, 0]^\top$  \\
			normalization $\mu$ & $[250, 50, 10, 5]$  &$[500, 50, 10, 5]$ & $[1000, 50, 15, 1]$ \\
			normalization $\sigma$ & $[150, 30, 5, 2]$ & $[300, 30, 5, 2]$ & $[600, 30, 6, 3]$
		\end{tabular}
		\caption{scenario data}
		\label{tab:scenario_data}
	\end{center}
\end{table}

Recall that policy iteration algorithms produce a sequence of value-functions and policies that approximate $V_*$ and $\pi_*$. Each iterate $V_{k+1}$ and $\pi_{k+1}$ is closer to $V_*$ and $\pi_*$ than its predecessor $V_k$ and $\pi_k$.
In the GPI algorithm, the iterative evaluation stops, if the values of all states change by less than the reward for one time step $\Delta t$ within one evaluation step. When this point is reached, only the time penalty propagates through the network. At states $s_t$, where the agent only moves slowly, chances are that the successor state $s_{t+1}$ is (roughly) equal to $s_t$ (and therefore $V(s_t)=V(s_{t+1})$). In such cases, the state value $V(s_t)$ is overwritten with $r_t + V(s_t)$ at each iteration. This goes on indefinitely if PE is not stopped manually. Every policy iteration where the evaluation step is repeated multiple times obviously takes longer than an optimistic policy iteration. The whole policy iteration algorithm has converged if - at the last policy improvement step - none of the actions have changed. This means that all actions were already optimal with respect to the current value function as well as the previous one. \smallbreak

Value iteration only iterates state value functions. It combines one step of policy evaluation and policy improvement in every iteration. One value iteration therefore usually takes less time than one step of policy iteration. If a close approximation of $V_*$ is found, a policy can be derived by acting greedily with respect to the last value function iterate.

\subsubsection{Scenario 1 - Coarse Discretization}

In scenario 1, the agent has to cover $500~\text{m}$ while flying through calm air. Table \ref{tab:2d_flight_data_500m} shows the flight times from the start-state to the goal after policy optimization with synchronous\footnote{Recall from chapter \ref{chapter2}, that synchronous algorithms update all state values simultaneously after a sweep over the complete state space. They usually are easier to implement and do not exploit the problem structure, but take longer to converge.} GPI, OPI and VI on a coarse grid (cf. Tab.~\ref{tab:grids}, column 1 (left)). This comparison has two goals: Investigating how the algorithms deal with a coarse grid and comparing calculation times. As a benchmark, the optimal control \nomenclature[A]{OC}{optimal control} is shown in the first column. Calculation time is the time it took to obtain the given trajectories and control sequences. Note that lower calculation and flight times are better.

Although all three DP algorithms theoretically take up to $|\mathcal{A}|^{|\mathcal{S}|}$ iterations to converge (c.f. section \ref{sec:PI}), they are known to typically converge after only few iterations. This is also the case in this scenario. All algorithms yield similar flight times that are significantly higher than the OC result. The main reason for this is the coarse state grid, i.e. poor approximation of the true value function. As can be seen in table \ref{tab:2d_flight_data_500m}, VI takes the least calculation time. As all DP algorithms converge to a reasonable solution, only optimistic policy iteration and value iteration are considered for further investigation. Calculation time gets more critical at higher distances, i.e. more grid points (cf. section~\ref{sec:curses}). Fig. \ref{tikz:vi500mcoarse} shows the trajectory obtained by synchronous VI in scenario 1. The thick line is the glider trajectory $[x(t),z(t)]^T$, the thin line is the control sequence $[x(t),\alpha(t)]^T$  The other algorithms yield similar trajectories, which can be found in appendix \ref{appendixE}.

\begin{table}[htb]
	\begin{center}
		\begin{tabular}{l|c c c c}
			 & OC & GPI & OPI & VI \\ \hline
			iterations (-) & - & 15 & 18 & 17 \\
			calculation time (h:min) & 0:15 & 3:15 & 1:54 & 1:42 \\
			flight time (s) & 19.7 & 21.2 & 21.2 & 21.2
		\end{tabular}
		\caption{Scenario 1, comparison of synchronous algorithms on a coarse grid}
		\label{tab:2d_flight_data_500m}
	\end{center}
\end{table}

To reduce computing time, OPI and VI have also been implemented as asynchronous algorithms (cf. section \ref{sec:prob_structure}). As can be seen in table~\ref{tab:2d_flight_data_500m_async}, the number of required iterations is significantly lower than in the synchronous case. Each iteration takes longer than in synchronous PI and VI, because splitting the state-set into layers and distributing the data to the CPU cores requires more effort here than in the synchronous case. The trajectories obtained by the asynchronous implementations are identical to their synchronous counterparts (cf. Fig.~\ref{tikz:vi500mcoarse},~\ref{tikz:gpi500mcoarse} and~\ref{tikz:opi500mcoarse}). They are therefore not presented here.

\begin{table}[htb]
	\begin{center}
		\begin{tabular}{l|c c c}
			& OC & OPI & VI \\ \hline
			iterations (-) & - & 6 & 4 \\
			calculation time (h:min) & 0:15 & 0:37 & 0:29  \\
			flight time (s) & 19.7 & 21.2 & 21.2
		\end{tabular}
		\caption{Scenario 1, comparison of asynchronous algorithms on a coarse grid}
		\label{tab:2d_flight_data_500m_async}
	\end{center}
\end{table}
\begin{table}[htb]
	\begin{center}
		\begin{tabular}{l|c c}
			& OC & VI \\ \hline
			iterations (-) & - & 7 \\
			calculation time (h:min) & 0:15 & 2:04 \\
			flight time (s) & 19.7 & 20.0
		\end{tabular}
		\caption{Scenario 2, performance of asynchronous VI on a fine grid}
		\label{tab:2d_flight_data_500m_async_fine_grid}
	\end{center}
\end{table}
\tikzsetnextfilename{optiandvi500mcoarse}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=500,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIfiveH};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVIfiveHcoarse};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIfiveH};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVIfiveHcoarse};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{OC}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{OC - control sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{VI}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{VI - control sequence}
	
	\end{axis}
	
	\foreach \gridrow in {}{
		\draw[color=gray] (0,0.409*\textheight - \gridrow*0.02045*\textheight) -- + (0.2*\textwidth-0.03*\textwidth*\gridrow,0);
	}
	\foreach \gridcol in {}{
		\draw[color=gray] (\gridcol*0.03*\textwidth,0.409*\textheight) -- + (0,-0.14*\textheight+0.02*\textheight*\gridcol);
	}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}
	\caption{Results of synchronous value iteration and optimal control in scenario 1}
	\label{tikz:vi500mcoarse}
\end{figure}

Figures~\ref{tikz:gpi500mcoarse} and~\ref{tikz:opi500mcoarse} in the appendix show the results of value iteration, generalized policy iteration and optimistic policy iteration, respectively.

\subsubsection{Scenario 1 - Fine Discretization}

In this section, the result of asynchronous value iteration on a fine state grid is shown. The number of points in the x- and z-directions are now 52 instead of 27. The speed vector discretization is like in the previous section.

The asynchronous VI algorithm works itself through the state space backwards, updating the state values close to the target first and moving backwards from there (cf. \ref{sec:prob_structure}). This increases convergence speed significantly. The number of required iterations is also independent of the number of states with this method. This is not the case with synchronous updates (i.e. updating all state values at once after a complete state space sweep). 

\tikzsetnextfilename{optiandvi500m}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=500,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIfiveH};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVIfiveHfine};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,\dataOPTIoneK
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIfiveH};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVIfiveHfine};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{OC}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{OC - control sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{VI}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{VI - control sequence}

	\end{axis}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}
	\caption{Results of value iteration and optimal control in scenario 1 with a fine grid}
	\label{tikz:vi500m}
\end{figure}

Figure \ref{tikz:vi500m} shows in blue the trajectory after only 7 value iterations. To smoothen the control sequence and trajectory, the results of VI have been used to train a neural network. The actions during the simulation are ANN outputs. As can be seen, both the policy from VI and the optimal control direct the agent towards the target. The differences between the control sequences are only minor, which leads to a difference in flight times of only 0.3 s. Although the trajectories differ more, the result is satisfactory.

\begin{table}[htb]
	\begin{center}
		\begin{tabular}{l|c c}
			algorithm & OC & VI \\ \hline 
			iterations (-) & - & 8 \\
			calculation time (h:min) & 1:31 &  \\
			flight time (s) & 47.0 & 49.8
		\end{tabular}
		\caption{Flight- and computation-times for OC, OPI and VI in scenario 2}
		\label{tab:2d_flight_data_1000m}
	\end{center}
\end{table}
\FloatBarrier

\subsubsection{Scenario 2}

In scenario 2, the agent has to cover a distance of 1000m, the start state and wind conditions are the same as in scenario 1. Fig.~\ref{tikz:vi1000m} shows the trajectory after 8 value iterations. The flight and calculation times can be found in table~\ref{tab:2d_flight_data_1000m}.

\tikzsetnextfilename{optiandvi1000m}
\begin{figure}[htb]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=1000,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIoneK};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVIoneK};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIoneK};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVIoneK};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{OC}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{OC - control sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{VI}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{VI - control sequence}
	
	\end{axis}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}	
	\caption{Results of value iteration and optimal control in scenario 2}
	\label{tikz:vi1000m}
\end{figure}
\FloatBarrier
\subsubsection{Scenario 3}

In scenario 3, the agent has to cover a distance of 2000m, the start state and wind conditions are the same as in scenario 1. Fig.~\ref{tikz:vi2000m} shows the trajectory after  value iterations. The flight and calculation times can be found in table~\ref{tab:2d_flight_data_2000m}.

\begin{table}[htb]
	\begin{center}
		\begin{tabular}{l|c c}
			algorithm & OC & VI \\ \hline 
			iterations (-) & - &  \\
			calculation time (h:min) &&  \\
			flight time (s) &  & 
		\end{tabular}
		\caption{Flight- and computation-times for OC, OPI and VI in scenario 3}
		\label{tab:2d_flight_data_2000m}
	\end{center}
\end{table}

\tikzsetnextfilename{optiandvi2000m}
\begin{figure}[htb]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=2000,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTItwoK};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVItwoK};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTItwoK};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVItwoK};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{OC}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{OC - control sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{VI}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{VI - control sequence}
	
	\end{axis}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}	
	\caption{Results of value iteration and optimal control in scenario 2}
	\label{tikz:vi1000m}
\end{figure}

\FloatBarrier

\newpage
\subsection{Policy Initialization for TRPO}
\label{sec:dp_init_trpo}
In the previous section, it was shown that value iteration is a viable alternative for optimal control in certain path planning scenarios. The goal of this section is to find out whether value iteration can also serve to accelerate convergence of a TRPO algorithm.

The parameters of a neural network are usually initialized randomly according to a specific probability distribution (cf. section~\ref{sec:weight_init}). The idea behind policy initialization by DP is to use a set of weights, that have already been optimized in a similar scenario, in order to speed up convergence of a subsequent training procedure.

The TRPO training procedure differs from the one in dynamic programming. Instead of sweeping over the complete state space and performing a one step lookahead at each point, TRPO samples complete trajectories.

The performance of TRPO can either be measured by flight times (like in section \ref{sec:results_dp}) or by the return per iteration. Figures~\ref{tikz:trpoglorotvi1000mAvg} and \ref{tikz:trpoglorotvi1000mMax} show the average and maximal return per iteration of TRPO, respectively. In this TRPO algorithm, an iteration consists of 25 episodes, i.e. 25 flights through the state space. After each iteration, the policy parameters are changed in order to increase the expected return. More information about TRPO can be found in \cite{DBLP:journals/corr/SchulmanLMJA15}. A detailed description of glider trajectory optimization through TRPO is contained in \cite{Zuern2017}.

The benchmark for DP pre-training is scenario 2. The agent has to cover 1000m through calm air and starts at a height of 100m with zero velocity.

The plots of the average and maximum return per iteration typically have an exponential shape. The average return has numerous downward peaks, which is due to the exploration aspect of TRPO. This is the desired behavior and is not affected by pre-training, as Fig.~\ref{tikz:trpoglorotvi1000mAvg} shows. The pre-trained policy performs better at early iterations, but the effect fades away after about 270 iterations.

\tikzsetnextfilename{trpoglorotvsvi1000mAvg}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.5\textwidth,
		height=0.3\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=300,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKAvgReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKAvgReturn};

	\end{axis}
		
	\end{tikzpicture}
	
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.5\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=800,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=-20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKAvgReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKAvgReturn};
	
	\addlegendimage{red,line width=0.8pt, mark=none}\addlegendentry{glorot uniform initialization}
	\addlegendimage{blue,line width=0.8pt, mark=none}\addlegendentry{value iteration}
	
	\end{axis}

	\end{tikzpicture}
	\caption{Average return per iteration with and without policy initialization}
	\label{tikz:trpoglorotvi1000mAvg}
\end{figure}

\tikzsetnextfilename{trpoglorotvsvi1000mMax}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.5\textwidth,
		height=0.3\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=300,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKMaxReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKMaxReturn};
	
	\end{axis}
	
	\end{tikzpicture}
	
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.5\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=800,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=-20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKMaxReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKMaxReturn};
	
	\addlegendimage{red,line width=0.8pt, mark=none}\addlegendentry{glorot uniform initialization}
	\addlegendimage{blue,line width=0.8pt, mark=none}\addlegendentry{value iteration}
	
	\end{axis}
	
	\end{tikzpicture}	
	\caption{Maximum return per iteration with and without policy initialization}
	\label{tikz:trpoglorotvi1000mMax}
\end{figure}

To see if TRPO spoils the policy's ability to find the goal from differing initial states, the agent is launched from other states than before and the results before and after TRPO are compared.
