% !TeX root = ../../../Main.tex
\chapter{Results}
\label{chapter6}

\section{2D Dynamic Programming}

\subsection{Optimal Control Benchmark}
\label{sec:results_dp}
In the 2D scenario, three Dynamic Programming algorithms are compared:

\begin{itemize}
	\item Generalized policy iteration (cf. section \ref{sec:PI})
	\item Optimistic policy iteration (cf. section \ref{sec:PI})
	\item Value iteration (cf. section \ref{subsection:VI})
\end{itemize}

The agent faces three different scenarios, each with calm air. Table \ref{tab:scenario_data} shows the distance to cover, start state, and the mean and standard deviation for state normalization (cf.~\ref{sec:input_standardization}) for each scenario. For each scenario, there is a separate mean $\mu$ and standard deviation $\sigma$ for each coordinate. Therefore, each column contains a vector of means and standard deviations.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|c c c}
			scenario & 1 & 2 & 3 \\ \hline
			distance $d_T$ & 500m & 1000m & 2000m\\
			start state $s_0$ & $s_0 = [0, 100, 0, 0]^\top$ &  $s_0 = [0, 100, 0, 0]^\top$  &  $s_0 = [0, 100, 0, 0]^\top$  \\
			normalization $\mu$ & $[250, 50, 10, 5]$  & $[500, 50, 10, 5]$ & $[1000, 50, 15, 1]$ \\
			normalization $\sigma$ & $[150, 30, 5, 2]$ & $[300, 30, 5, 2]$ & $[600, 30, 6, 3]$
		\end{tabular}
		\caption{scenario data}
		\label{tab:scenario_data}
	\end{center}
\end{table}

Recall that policy iteration algorithms produce a sequence of value-functions and policies that approximate $V_*$ and $\pi_*$. Each iterate $V_{k+1}$ and $\pi_{k+1}$ is closer to $V_*$ and $\pi_*$ than its predecessor $V_k$ and $\pi_k$.
In the GPI algorithm, the iterative evaluation stops, if the values of all states change by less than the reward for one time step $\Delta t$ within one evaluation step. When this point is reached, only the time penalty propagates through the network. At states $s_t$, where the agent only moves slowly, chances are that the successor state $s_{t+1}$ is (roughly) equal to $s_t$ (and therefore $V(s_t)=V(s_{t+1})$). In such cases, the state value $V(s_t)$ is overwritten with $r_t + V(s_t)$ at each iteration. This goes on indefinitely if PE is not stopped manually. Every policy iteration where the evaluation step is repeated multiple times obviously takes longer than an optimistic policy iteration. The whole policy iteration algorithm has converged if - at the last policy improvement step - none of the actions have changed. This means that all actions were already optimal with respect to the current value function as well as the previous one. \smallbreak

Value iteration only iterates state value functions. It combines one step of policy evaluation and policy improvement in every iteration. One value iteration therefore usually takes less time than one step of policy iteration. If a close approximation of $V_*$ is found, a policy can be derived by acting greedily with respect to the last value function iterate.

In scenario 1, the agent has to cover $500~\text{m}$ while flying through calm air. Table \ref{tab:2d_flight_data_500m} shows the flight times from the start-state to the goal after policy optimization with GPI, OPI and VI. As a benchmark, the optimal control \nomenclature[A]{OC}{optimal control} is shown in the first column. Calculation time is the time it took to obtain the given trajectories and control sequences. Note that lower calculation and flight times are better. As mentioned before, value iteration takes the least time for one iteration and converges before OPI and GPI.

Although all three DP algorithms theoretically take up to $|\mathcal{A}|^{|\mathcal{S}|}$ iterations to converge (c.f. section \ref{sec:PI}), they are known to typically converge after only few iterations. This is also the case in this scenario.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|c c c c}
			 & OC & GPI & OPI & VI \\ \hline
			iterations (-) & - & & & 7 \\
			calculation time (h:min) & 0:15 & & & 2:04 \\
			flight time (s) & 19.7 & & & 20.0
		\end{tabular}
		\caption{Flight- and computation-times for OC, GPI, OPI and VI in scenario 1}
		\label{tab:2d_flight_data_500m}
	\end{center}
\end{table}

All algorithms yield similar flight times that are slightly higher than the OC result. As can be seen in table \ref{tab:2d_flight_data_500m}, VI takes the least calculation time. As all algorithms converge to a reasonable solution, both policy iteration algorithms are not considered for scenarios 2 and 3, because VI converges faster and calculation time gets more critical at higher distances, i.e. more grid points. 

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/opti500m-coarse.dat}}\dataOPTIfiveH

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/opti1000m-coarse.dat}}\dataOPTIoneK

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no68-i27-500m-dt250ms.dat}}\dataOPIfiveH

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no60-i18-500m-dt250ms.dat}}\dataGPIfiveH

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no108-i7-500m-dt250ms.dat}}\dataVIfiveH

\pgfplotstableread[col sep=comma] {\detokenize{./src/pics/tikz/dat-no104-i0-1000m-dt250ms.dat}}\dataVIoneK

\pgfplotstableread[col sep=comma]
{\detokenize{./src/pics/tikz/trpo-glorot-vi-1km-AvgRet.dat}}\trpoGlorotVsVIoneKAvgReturn

\pgfplotstableread[col sep=comma]
{\detokenize{./src/pics/tikz/trpo-glorot-vi-1km-MaxRet.dat}}\trpoGlorotVsVIoneKMaxReturn

\tikzsetnextfilename{optiandgpi500m}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=500,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIfiveH};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataGPIfiveH};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIfiveH};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataGPIfiveH};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{OC}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{OC - control sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{GPI}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{GPI - control sequence}
	
	\end{axis}
	
	\foreach \gridrow in {}{
		\draw[color=gray] (0,0.409*\textheight - \gridrow*0.02045*\textheight) -- + (0.2*\textwidth-0.03*\textwidth*\gridrow,0);
	}
	\foreach \gridcol in {}{
		\draw[color=gray] (\gridcol*0.03*\textwidth,0.409*\textheight) -- + (0,-0.14*\textheight+0.02*\textheight*\gridcol);
	}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}
	\caption{Results of generalized policy iteration (blue) and optimal control (red) in scenario 1}
	\label{tikz:gpi500m}
\end{figure}

Figures~\ref{tikz:gpi500m},~\ref{tikz:opi500m} and~\ref{tikz:vi500m} show the results of generalized policy iteration, optimistic policy iteration and value iteration, respectively. As a benchmark, every figure also contains the optimal trajectory and control sequence.

The value iteration algorithm clearly achieves the best result with respect to flight time. Within the first 30m, the angle of attack is a little higher than optimal, leading to a trajectory that lies above the optimal one. Nevertheless, the flight time is only 0.3s higher than the benchmark. Because the state values are updated 

Figure \ref{tikz:gpi500m} shows the agent's trajectory optimized with GPI. Optimization took approximately two hours (c.f. table \ref{tab:2d_flight_data_500m}). Qualitatively, the control sequence is very similar to the optimal control sequence. Within the first 50m of horizontal movement, there is some stutter in the control sequence which increases flight time slightly. 

In the upper left corner, a small part of the state grid is drawn. The grid elements have their original size with respect to the height and the distance.

Fig. \ref{tikz:vi500m} shows the trajectory and control sequence after only 7 value iterations.
The algorithm work themselves through the state space backwards, updating the state values close to the target first and moving backwards from there (cf. \ref{sec:prob_structure}). This increases convergence speed significantly. The number of required iterations is also independent of the number of states with this method. This is not the case with synchronous updates (i.e. updating all state values at once as soon as there is a new value for all states). 

\tikzsetnextfilename{optiandopi500m}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=500,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIfiveH};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPIfiveH};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIfiveH};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataOPIfiveH};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{Optimal Trajectory}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{Optimal Control Sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{Optimistic Policy Iteration: Trajectory}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{Optimistic Policy Iteration: Control Sequence}
	\end{axis}
	
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};


	\end{tikzpicture}
	\caption{Results of optimistic policy iteration (blue) and optimal control (red) in scenario 1}
	\label{tikz:opi500m}
\end{figure}

\tikzsetnextfilename{optiandvi500m}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=500,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIfiveH};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVIfiveH};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,\dataOPTIoneK
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIfiveH};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVIfiveH};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{Optimal Trajectory}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{Optimal Control Sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{Value Iteration: Trajectory}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{Value Iteration: Control Sequence}

	\end{axis}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}
	\caption{Results of value iteration (blue) and optimal control (red) in scenario 1}
	\label{tikz:vi500m}
\end{figure}

Figure \ref{tikz:vi1000m} shows in blue the trajectory in scenario 2 after 7 value iterations. The thick line is the glider trajectory $[x(t),z(t)]^T$, the thin line is the control sequence $[x(t),\alpha(t)]^T$ The agent is dropped at the start state with zero velocity. As can be seen, both the policy from VI and the optimal control (red) direct the agent towards the target.

\begin{table}[h]
	\begin{center}
		\begin{tabular}{r|c c c c}
			algorithm & OC & VI \\ \hline 
			approx. time per iteration (s) & - & \\
			iterations (-) & - & 43 \\
			calculation time (h:min) & 1:31 & \\
			flight time (s) & 47.0 & 50.4
		\end{tabular}
		\caption{Flight- and computation-times for OC, OPI and VI in scenario 2}
		\label{tab:2d_flight_data_1000m}
	\end{center}
\end{table}

Both algorithms yield policies that are able to find the goal from various initial states. 

\tikzsetnextfilename{optiandvi1000m}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=1000,
		legend pos=north east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=0, ymax=100,
	axis y line*=left,
	xlabel={distance (m)},
	xlabel near ticks,
	ylabel={height $-z$ (m)},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataOPTIoneK};
	\addplot[blue, line width=1.2pt, mark=none,forget plot] table[x index=0, y index=1]{\dataVIoneK};
	\end{axis}
	
	\begin{axis}[
	clip mode=individual,
	ymin=0, ymax=0.2,
	axis y line*=right,
	xtick=\empty,
	xlabel=\empty,
	%xlabel near ticks,
	ylabel={$\alpha$ (rad)},
	ylabel near ticks,
	]
	% aircraft controls
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=2]{\dataOPTIoneK};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=4]{\dataVIoneK};
	
	\addlegendimage{red,line width=1.2pt, mark=none}\addlegendentry{Optimal Trajectory}
	\addlegendimage{red,line width=0.5pt, mark=none}\addlegendentry{Optimal Control Sequence}
	\addlegendimage{blue,line width=1.2pt, mark=none}\addlegendentry{Value Iteration: Trajectory}
	\addlegendimage{blue,line width=0.5pt, mark=none}\addlegendentry{Value Iteration: Control Sequence}
	
	\end{axis}
	\node[rotate=-90, xshift=-.2cm, yshift=.1cm, fill=none] (glider) at (0,0.409*\textheight) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}	
	\caption{Results of value iteration (blue) and optimal control (red)}
	\label{tikz:vi1000m}
\end{figure}

\subsection{Policy Initialization for TRPO}
\label{sec:dp_init_trpo}
In the previous section, it was shown that value iteration is a viable alternative for optimal control in certain path planning scenarios. The goal of this section is to find out whether value iteration can also serve to accelerate convergence of a TRPO algorithm.

The parameters of a neural network are usually initialized randomly according to a specific probability distribution (cf. section~\ref{sec:weight_init}). The idea behind policy initialization by DP is to use a set of weights, that have already been optimized in a similar scenario, in order to speed up convergence of a subsequent training procedure.

The TRPO training procedure differs from the one in dynamic programming. Instead of sweeping over the complete state space and performing a one step lookahead at each point, TRPO samples complete trajectories. A specific number of trajectories 

The performance of TRPO can either be measured by flight times (like in section \ref{sec:results_dp}) or by the average and maximal return

\tikzsetnextfilename{trpoglorotvsvi1000mAvg}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.5\textwidth,
		height=0.4\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=300,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKAvgReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKAvgReturn};

	\end{axis}
		
	\end{tikzpicture}
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=800,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=-20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKAvgReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKAvgReturn};
	
	\addlegendimage{red,line width=0.8pt, mark=none}\addlegendentry{glorot uniform initialization}
	\addlegendimage{blue,line width=0.8pt, mark=none}\addlegendentry{value iteration}
	
	\end{axis}

	\end{tikzpicture}	
	\caption{Average return per iteration with and without policy initialization}
	\label{tikz:trpoglorotvi1000mAvg}
\end{figure}

\tikzsetnextfilename{trpoglorotvsvi1000mMax}
\begin{figure}[hbt]
	\centering
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.5\textwidth,
		height=0.4\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=300,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKMaxReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKMaxReturn};
	
	\end{axis}
	
	\end{tikzpicture}
	\begin{tikzpicture}
	
	\pgfplotsset{
		width=0.85\textwidth,
		height=0.65\textwidth, 
		xmajorgrids, 
		ymajorgrids, 
		enlarge x limits=false,
		scaled x ticks = false,
		x tick label style={/pgf/number format/.cd, fixed, set thousands separator={}},
		scaled y ticks = false,
		yticklabel style={/pgf/number format/.cd, fixed, set thousands separator={}},
		xmin=0, xmax=800,
		legend pos=south east,
		legend cell align=left
	}
	
	\begin{axis}[
	% hide x-axis once
	% grid andeuten (andere Farbe)
	clip mode=individual,
	ymin=-20, ymax=60,
	axis y line*=left,
	xlabel={iteration},
	xlabel near ticks,
	ylabel={Average Return},
	ylabel near ticks,
	]	
	% aircraft trajectory
	\addplot[red, line width=0.5pt, mark=none,forget plot] table[x index=0, y index=1]{\trpoGlorotVsVIoneKMaxReturn};
	\addplot[blue, line width=0.5pt, mark=none,forget plot] table[x index=0, y 
	index=2]{\trpoGlorotVsVIoneKMaxReturn};
	
	\addlegendimage{red,line width=0.8pt, mark=none}\addlegendentry{glorot uniform initialization}
	\addlegendimage{blue,line width=0.8pt, mark=none}\addlegendentry{value iteration}
	
	\end{axis}
	
	\end{tikzpicture}	
	\caption{Maximum return per iteration with and without policy initialization}
	\label{tikz:trpoglorotvi1000mMax}
\end{figure}

