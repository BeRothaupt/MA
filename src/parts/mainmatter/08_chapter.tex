% !TeX root = ../../../Main.tex
\chapter{Conclusion and Outlook}
\label{chapter8}
In this work, three dynamic programming algorithms have been implemented and used for glider trajectory optimization in a calm environment. Their results were compared to an optimal control result and it has been investigated, whether pre-training with DP helps convergence of a TRPO algorithm. The three algorithms all converged to a near-optimal solution in a scenario where 500m were to cover and the initial state was known a priori. Value iteration took the least time and was therefore chosen for studies in scenarios with distances of 1000m and 2000m. Results showed that TRPO performance is improved by DP. But if the pre-training time had been used for more TRPO iterations instead, the total calculation time would have been even lower.

Based on the results in chapter~\ref{chapter6}, one could argue, that dynamic programming does not make sense in this context. With a sufficiently fine state grid, even value iteration takes eight times as long as the optimal control benchmark in scenario 1 and only yields a suboptimal result in all scenarios. If compared to TRPO, a policy optimization algorithm (cf.~\cite{Zuern2017}), it performs worse with respect to calculation time as well as flight time. It also cannot deal with unknown upwinds, which TRPO can.

A big advantage of DP is however, that it inherently covers the whole state space. If the result of one of the DP algorithms is used to train an artificial neural network, an agent utilizing it can get to the goal from every point in the state space\footnote{This is only true, if reaching the goal is physically possible at all. A glider can, for example, not reach a point that is 1000m away, if its altitude is insufficient and there are no upwinds.}. This is not true for TRPO and OC, which both optimize a specific trajectory from a specific start state to the goal. As a tool for policy initialization, DP is especially helpful, if the start state and therefore the approximate trajectory is not known a priori.

Even if the start state is known, DP has an impact on the results of TRPO. As the results in Fig. \ref{tikz:trpoglorotvi1000mMax} show, pre training with a DP algorithm increases the initial average and maximum return per iteration for a TRPO algorihm. If the stopping criterion of TRPO is reaching a specific average return (i.e. a specific average flight time), Fig. \ref{tikz:trpoglorotvi1000mAvg} shows that this goal can be reached earlier than with standard TRPO and a Glorot-initialized policy ANN. For example, a pre-trained policy net reaches an average return of $50$ after only 80 iterations, while a Glorot-initialized policy net takes about 130 iterations. Whether this higher return in early TRPO iterations justifies the additional expense of performing a DP pre-training, depends on the performance of the TRPO algorithm and can therefore not be answered generally. A DP result can also be calculated only once and then stored and used for multiple TRPO calculations afterwards (e.g. with random updrafts or horizontal winds). This way, the time DP takes carries less weight if compared to TRPO without pre-training.

One of the greatest weaknesses of DP is the curse of dimensionality. To mitigate this, the state space can be restricted by excluding states, that are obviously not relevant, to reduce computational expense. This could lead to a compromise between TRPO and traditional DP, where some of the generality of a DP solution is retained, while computation time is reduced. Alternatively, shrinking the state space could make finer grids feasible, improving the results of DP.

During this work, neural networks have also been tested as function approximators representing the policy and state value function during calculation. The output of a neural network does generally not match the training data exactly, which can jeopardize convergence of the algorithm. The DP calculations, where neural networks were used, did in fact not converge to satisfactory solutions. This is why tables were used to store greedy actions and state values. They yielded better results, but a continuous representation of the state value function and policy is generally more desirable than a discrete one, especially in continuous environments. A continuation of this work could include trying to incorporate neural networks into the DP algorithm while retaining the convergence properties.

As for the general idea of pre-training a policy, it was shown that this can improve the results of TRPO. By using true knowledge about the model or real flight data, the results could be improved even further.

Another possible continuation of this work includes using VI to find optimal trajectories in 3D scenarios. This thesis shows, that - in principle - DP is suitable for obtaining a policy that guides an agent to a goal in 2D. The main challenge of implementing VI in a 3D scenario is the increase in state variables and the resulting increase in calculation time (cf. section \ref{sec:curses}). There are already methods that try to alleviate the curses of dimensionality, as for example in \cite{Powell2007ADP}. Within this work, the required code to optimize a trajectory in 3D has already been written.

The main goal of this work was to investigate whether DP is suitable for the presented application and whether it improves the performance of a subsequent optimization by RL. With respect to these goals, the results can be regarded as a success.
