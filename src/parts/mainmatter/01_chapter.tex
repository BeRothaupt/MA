% !TeX root = ../../../Main.tex

\chapter{Introduction}

The goal of this work is to find an optimal trajectory of a glider moving through calm air and to reach a given distance in minimal time. The resulting optimal control policy can be used as a starting point for training the glider on scenarios with different wind conditions utilizing other algorithms like Reinforcement Learning (RL). The basic idea is to use the knowledge about the glider dynamics to pre-train a policy. After that, an RL algorithm is used to train the policy on scenarios with the agent facing a stochastic environment.

In soaring competitions, there are two main goals that pilots must be able to achieve in order to be successful: exploiting updrafts to gain potential energy and covering a given distance in minimal time. These are two conflicting tasks. The longer a pilot decides to stay within an updraft, the more energy can he harvest. The time spent there however increases the total flight time which interferes with the other goal. Generally, the locations where updrafts occur are not known a-priori. So a pilot has to react spontaneously to changes in his vicinity in order to harvest as much energy as possible, which takes a lot of experience. Automation of the optimal-flying task is therefore challenging.

Another topic, where using energy from updrafts can be helpful, is electric flying. Electric aircraft have been an important research topic in recent years. They can operate environmentally friendly and are easy to maintain. Their range and endurance is - however - limited by the energy density of current batteries. This means they either have to land frequently to recharge or be equipped with solar panels on their wings which weigh them down and are difficult to maintain. Electric UAVs suffer from the same deficiency. They can theoretically be used for a wide range of applications from gathering scientific data to surveillance with battery capacity being their only limiting factor. The exploitation of thermal updrafts is one way to increase the endurance of an aircraft without changing anything on the airframe itself, which makes it relatively cheap to implement.

Reacting to changes in the environment is a topic that is inherently covered by various Machine Learning algorithms. In fact, the most challenging and therefore interesting and powerful aspect of \textit{learning} is gaining the ability to generalize and behave reasonably in new situations, such as unknown updraft distributions. This sets Machine Learning apart from traditional optimal control algorithms that rely on precise knowledge of the underlying problem.

Finding an optimal flight path with respect to varying mission goals in unknown atmosphere is a challenging task. While the dynamics of the aircraft itself are known and understood, the occurrence of updrafts cannot be predicted with sufficient accuracy. RL algorithms are very good at finding an optimal path through an unknown environment. They require no prior knowledge and learn from experience they gather through interacting with the environment. The stochastics subject to the likelihood of occurence of thermal updrafts and their specific characteristics are hard to model. Sampling from experience constitutes a way to learn how to act, while facing possibly hidden stochastics within the environment. This way, RL algorithms can deal with apparently random updrafts efficiently.

Markus Zuern applied Trust Region Policy Optimization (TRPO) and Asynchronous Advantage Actor Critic (A3C) to a trajectory optimization problem~\cite{Zuern2017} (some of the topics of \cite{Zuern2017} are also covered in \cite{Notter2018}). In his work, both algorithms deal with the complete environment, i.e. the glider dynamics and the stochastic wind distribution, at the same time. They require no knowledge about the environment, nor do they try to learn a model. Instead, they directly optimize a policy. In TRPO, there does not even exist a value function that represents known information about the MDP. Although the results in \cite{Zuern2017} are remarkable, they do not exploit the fact that one part of the trajectory optimization MDP is known a priori: the glider dynamics when flying through calm air.

In \cite{ReddyE4877}, the authors combine simulations of atmospheric flow with reinforcement learning algorithms to find strategies for gliders to cope with and exploit turbulent thermal updrafts. They utilize a model-free SARSA algorithm to train the agents. The resulting policies are able to find thermal updrafts in a nonstationary turbulent 3D environment and exploit them more or less conservatively, depending on the intensity of the turbulence.

The goal of this work is to utilize this knowledge by applying Dynamic Programming to calculate a time-optimal trajectory through calm air. This is achieved by first finding an optimal policy and following it. The resulting policy is then improved with TRPO. This is similar to the behavior one would expect from a human pilot. If he/she is confronted with unknown updrafts, he/she would at first also rely on his experience from prior flights, be it through calm air or not. Dynamic Programming (DP)\nomenclature[A]{DP}{Dynamic Programming} is one of the most popular algorithms for MDPs with a state space that is not too large. Typically, MDPs with up to a few millions of states can be solved with DP algorithms.

Dynamic Programming is a general term that summarizes a certain kind of algorithms. These algorithms can be used to solve problems that can be split into subproblems. The optimal solution of each subproblem can then be used to recompose the optimal solution to the whole problem. The underlying principle is called the \textit{principle of optimality}. The term was first used by Richard E. Bellman in 1954 \cite{Bellman1954}. He proved that various optimization problems could be solved efficiently by making use of the principle of optimality instead of traditional optimal control theory. In his book \textit{Applied Dynamic Programming}, Bellman covered numerous examples of optimal control problems and shows how they can be solved by Dynamic Programming \cite{Bellman1962}. 
If the subproblems are similar, DP is especially effective. DP however suffers from the "curse of dimensionality", that is in large or continuous state or action spaces it quickly becomes inefficient \cite{Powell2007ADP}. The obvious approach is to discretize the state space and deal with a smaller number of states. If done properly, this yields an approximation of the exact optimal solution. The achieved approximate solution is close to the exact one if the discretization is sufficiently fine. Like in many technical topics, there is a tradeoff between precision and cost.

Dynamic Programming algorithms have already been used to calculate optimal trajectories for airborne agents. Richard Bellman showed in \cite[chapter~VI]{Bellman1962}, that DP can be used to solve various optimal control problems, including satellite control and finding a time-optimal airplane climb procedure. In \cite{Wirth2015MeteorologicalPP}, dynamic programming is used to calculate an optimal trajectory for a solar powered UAV across the atlantic ocean in consideration of meteorological conditions. Passenger aircraft trajectories have also been investigated, as for example in \cite{HARADA2013441}, where an optimal flight path from Tokyo to Fukuoka was found through dynamic programming.

In this work, a time optimal flight path through calm air is calculated with dynamic programming. First, the state and action space are discretized in section \ref{sec:disc2d}. A policy iteration (PI) algorithm is then used to calculate optimal paths in the vertical plane and in a 3D environment. The usefulness of the results from the 3D scenario is obvious. If airspace restrictions or topography restrict the horizontal trajectory, any 3D scenario degenerates to a 2D optimization of a vertical trajectory along the predefined ground trace. Therefore, the results of optimization in the vertical plane shown in section \ref{chapter6} are also relevant.

The first part of this thesis presents the general concepts and terms of Reinforcement Learning (cf. chapter \ref{chapter2}) and the theory of Dynamic Programming and the algorithms used in this work (cf. chapter \ref{chapter3}). Chapter \ref{chapter4} gives a brief overview of artificial neural networks (ANNs) and how training an ANN works. The trajectory optimization problem is described in chapter \ref{chapter5} and the results are presented and compared to an optimal control result in chapter \ref{chapter6}. The last chapter contains a brief discussion of the results and how using an ANN, that has been pre-trained by DP, for TRPO affects the results.