% !TeX root = ../../../Main.tex

\chapter{Introduction}

The goal of this work is to find an optimal trajectory of a glider moving through calm air and reaching a given distance in minimal time. The resulting optimal policy can be used as a starting point for training the glider on scenarios with different wind conditions utilizing other algorithms like reinforcement learning. The basic idea is to use the knowledge about the glider dynamics to pre-train a policy. After that, a reinforcement learning algorithm is used to train the policy on scenarios with the agent facing a stochastic environment.

In soaring competitions, there are two main goals that pilots must be able to achieve in order to be successful, exploiting updrafts to gain potential energy and covering a given distance in minimal time. These are two conflicting tasks. The longer a pilot decides to stay within an updraft, the more energy can he harvest. The time spent there however increases the total flight time which interferes with the other goal. Generally, the locations where updrafts occur are not known a-priori. So a pilot has to react spontaneously to changes in his vicinity in order to harvest as much energy as possible, which takes a lot of experience. Automation of the optimal-flying task is therefore challenging.

Reacting to changes in the environment is a topic that is inherently covered by various Machine Learning algorithms. In fact, the whole point of \textit{learning} is gaining the ability to generalize and behave reasonably in new situations, such as unknown updraft distributions. There are numerous algorithms that can learn from experience gathered through simulations or flight data records.

Another topic, where using energy from updrafts can be helpful, is electric flying. Electric aircraft have been an important research topic in recent years. They can operate environmentally friendly and are easy to maintain. Their range and endurance is - however - limited by the energy density of current batteries. This means they either have to land frequently to recharge or be equipped with solar panels on their wings which weigh them down and are difficult to maintain. Electric UAVs suffer from the same deficiency. They can theoretically be used for a wide range of applications from gathering scientific data to surveillance with battery capacity being their only limiting factor. The exploitation of thermal updrafts is one way to increase the endurance of an aircraft without changing anything on the airframe itself, which makes it relatively cheap to implement.

Finding an optimal flight path with respect to varying mission goals in unknown atmosphere is a challenging task. While the dynamics of the aircraft itself are known and understood, the occurrence of updrafts cannot be predicted with sufficient accuracy. Reinforcement Learning algorithms are very good at finding an optimal path through an unknown environment. They require no prior knowledge and learn from experience they gather through interacting with the environment. The stochastics subject to the likelihood of occurence and the specific characteristics are hard to model. Sampling from experience constitutes a way to learn how to act, while facing (hidden) stochastics within the environment.  This way, RL algorithms can deal with random updrafts efficiently.

Finding an optimal path through calm air can be done more efficiently, utilizing the knowledge about how a glider behaves in calm air. Flying a glider can be regarded as a decision process where the pilot decides what control surface position is suitable for the current situation. By varying, for example, the elevator position, the pilot can control the speed of the glider and the flight path angle. In reality, this process is time-continuous. Computers - however - can only deal with discrete processes. Therefore it is common practice to introduce a time step $\Delta t$. At each moment in time $t_k = t_0 + k\Delta t$, an elevator position is chosen and is kept constant until $t_{k+1}$. The smaller the time step, the better the approximation of a continuous process. Each state $s_t$ in this decision process has the Markov property, i.e. the information needed to calculate $s_{t+1}$ is fully contained in $s_t$. A decision process where the Markov property applies to all states is called a Markov Decision Process (MDP)\nomenclature[A]{MDP}{Markov Decision Process}. Dynamic Programming (DP)\nomenclature[A]{DP}{Dynamic Programming} is one of the most popular algorithms for MDPs with up to a few millions of states. (nochmal Ã¼berarbeiten)

Dynamic Programming is a general term that applies to problems that can be split into subproblems. The optimal solution of each subproblem can then be used to recompose the optimal solution to the whole problem. If the subproblems are similar, DP is especially effective. DP however suffers from the "curse of dimensionality", that is in large or continuous state spaces it quickly becomes inefficient. The obvious approach is to discretize the state space and deal with a smaller number of states. If done properly, this yields an approximation of the exact optimal solution. The achieved approximate solution is close to the exact one if the discretization is sufficiently fine. Like in many technical topics, there is a tradeoff between precision and cost.

In this work, a time optimal flight path through calm air is calculated with dynamic programming. First, the state and action space are discretized. A policy iteration algorithm is then used to calculate optimal paths in the vertical plane and in a 3D environment. The usefulness of the results from the 3D scenario is obvious. If airspace restrictions or topography restrict the horizontal trajectory, any 3D scenario degenerates to a 2D optimization of a vertical trajectory along the predefined ground trace.

In this work, Dynamic Programming is used to calculate a time-optimal trajectory through calm air by finding an optimal policy and then following it. This is possible with DP because the dynamics of a glider in calm air are known which is a requirement of DP.
This optimal policy is then used as a starting point for trajectory optimization in an unknown scenario. Essentially, a pilot who is confronted with unknown updrafts would at first also rely on his experience from prior flights, be it through calm air or not.
 