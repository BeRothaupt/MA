% !TeX root = ../../../Main.tex
\chapter{Trajectory Optimization with Dynamic Programming}
\label{chapter5}
\section{2D Environment}

In the context of this work, the aircraft is treated as a mass point. Rotational dynamics are neglected. This is sufficient for trajectory optimization algorithms. See \cite{Fichter2009} for more details. 


In this scenario, the glider moves in the geodetic vertical plane. Its control is the angle of attack $\alpha$. When $\alpha$ is increased, the lift $L$ increases. This results in an increase of $\gamma$, as can be seen in equation \ref{eq:dotgamma}. This way the agent can control its velocity vector and therefore its position. All parameters of the glider are shown in appendix \ref{appendix_A}. Any 3D-Scenario where the horizontal trajectory is restricted by obstacles or the optimal ground trace is obvious can be regarded as a 2D problem. The only control variable to be optimized is then the angle of attack or an equivalent quantity (e.g. vertical acceleration).

\subsection{Policy Representation}

As the Policy Iteration algorithm is running on a rectangular grid and the states that are updated do not change, a table that stores the greedy action for every state is sufficient for the Policy Iteration algorithm. Once the optimal policy $\pi_{calm}^*$ is found, a neural network is trained to approximate the optimal action for each of the states on the grid. This policy is then used to do TRPO.

\subsection{Equations of Motion}

The physical state of the glider in the geodetic vertical plane consists of the position $\boldsymbol{{}_g r}=[x,z]^\top$ and velocity $\boldsymbol{{}_g v_K}=[{}_g u_K,{}_g w_K]^\top$. Apart from the geodetic reference frame (index g), a body fixed frame (index f), a trajectory fixed frame (index k) and an aerodynamic frame (index a) are used to describe the glider dynamics. In the vertical plane, coordinates can be transformed between the frames by a rotation around the y-axis. Figure \ref{fig:coords2d} illustrates the transformation angles that connect the reference frames. Note that, in this work, there is no wind. $\vec{v}_W$ and $\alpha_W$ are therefore both zero and the aerodynamic and trajectory fixed frames are the same.

\begin{figure}[h]
	\centering
	\tikzsetnextfilename{coords2d}
	\tikzsetnextfilename{verticalPlane}
	\begin{tikzpicture}

% draw glider
\node[rotate=36, xshift=-.5cm, yshift=.2cm] (glider) at (0,0) {\includegraphics{./src/pics/tikz/simpleGlider.pdf}};

% draw coordinate frames
\node[draw, circle, fill=black, inner sep=0pt, minimum size=3.5pt] at (0,0) (o) {};

\draw[->] (o) -- (0:6cm) node[at end, below] {$x_g$};
\draw[->] (o) -- (-90:5cm) node[at end, below] {$z_g$};

\draw[->] (o) -- (36:6cm) node[at end, below] {$x_f$};
\draw[->] (o) -- (306:5cm) node[at end, below] {$z_f$};

\draw[->] (o) -- (12:6cm) coordinate (xa) node[at end, below] {$x_a$};
\draw[->] (o) -- (282:5cm) node[at end, below] {$z_a$};

\draw[->] (o) -- (24:6cm) node[at end, below] {$x_k$};
\draw[->] (o) -- (294:5cm) node[at end, below] {$z_k$};

% draw velocities
\draw[-latex, line width=1pt, UniSMittelblau] (o) -- (12:5.5cm) node[below, UniSMittelblau] (va) {$\vec{v}_A$};
\draw[-latex, line width=1pt, UniSAnthrazit] (o) -- (24:5.3cm) node[above, UniSAnthrazit] (vk) {$\vec{v}_K$};
\draw[-latex, line width=1pt, UniSHellblau] (12:5.5cm) -- (24:5.3cm) node[midway, right, UniSHellblau] {$\vec{v}_W$};

% draw forces
\draw[-latex, line width=1pt, UniSMittelblau] (o) -- ($(o)!4cm!90:(xa)$) coordinate (lift) node[above, UniSMittelblau] {$\vec{f}_L$};
\draw[-latex, line width=1pt, UniSMittelblau] (o) -- ($(o)!2cm!180:(xa)$) coordinate (drag) node[left, UniSMittelblau] {$\vec{f}_D$};
\draw[-latex, line width=1pt, UniSAnthrazit] (o) -- (270:3cm) node[left, UniSAnthrazit] {$\vec{f}_G$};

% draw rectangle signs
\draw [UniSMittelblau] (12:.5cm) arc (12:102:.5cm) node[pos=5.5/10, below, UniSMittelblau] {$\cdot$};
\draw [UniSMittelblau] (102:.4cm) arc (102:192:.4cm) node[pos=3.5/10, below, UniSMittelblau] {$\cdot$};

%draw help lines
\draw[dotted] (lift) -- ($(lift)!2cm!-90:(o)$) coordinate (intersect);
\draw[dotted] (drag) -- ($(drag)!4cm!90:(o)$);
\draw[-latex, line width=1pt, UniSMittelblau] (o) -- (intersect) node[left, UniSMittelblau] {$\vec{f}_A$};

% draw angles, colored in x-axes
\draw[-latex, line width=1pt, UniSHellblau] (24:3cm) arc (24:12:3cm) node[midway, left, yshift=-.2cm, UniSHellblau] {$\alpha_W$};
\draw[-latex, line width=1pt, UniSMittelblau] (12:3.5cm) arc (12:36:3.5cm) node[near end, left, yshift=-.2cm, UniSMittelblau] {$\alpha$};

\draw[-latex, line width=1pt, UniSMittelblau] (0:4cm) arc (0:12:4cm) node[midway, left, yshift=-.1cm, UniSMittelblau] {$\gamma_A$};
\draw[-latex, line width=1pt, UniSAnthrazit] (0:4.5cm) arc (0:24:4.5cm) node[near end, left, yshift=-.1cm, UniSAnthrazit] {$\gamma$};

% draw angles, grey in z-axes
\draw[-latex] (294:3cm) arc (294:282:3cm) node[pos=6/10, above] {$\alpha_W$};
\draw[-latex] (282:3.5cm) arc (282:306:3.5cm) node[pos=6.5/10, above] {$\alpha$};
\draw[-latex] (270:4cm) arc (270:282:4cm) node[midway, above] {$\gamma_A$};
\draw[-latex] (270:4.5cm) arc (270:294:4.5cm) node[near end, above] {$\gamma$};

\end{tikzpicture}
	\caption{Reference frame transformation~\cite{Notter2018}}
	\label{fig:coords2d} 
\end{figure}


In the vertical plane, there are 4 state variables, $x$, $z$, $u=\dot{x}$ and $w=\dot{z}$, each with respect to a geodetic reference frame that has its origin at a point on the earth surface. The aerodynamic forces are often expressed in an aerodynamic reference frame. The coordinate transformations $u$ and $w$ can also be expressed by $V$ and $\gamma$.

The dynamics in the vertical plane are given by the following equations. They are taken from \cite{Fichter2009}.

\begin{equation}
\dot{x} = V \; cos(\gamma)
\end{equation}

\begin{equation}
\dot{z} = - V \; sin(\gamma)
\end{equation}

\begin{equation}
\dot{V} = -\frac{D + g \; cos(\gamma)}{m}
\end{equation}

\begin{equation}
\dot{\gamma} = \frac{L}{V \; m} - \frac{g \; cos(\gamma)}{V} 
\label{eq:dotgamma}
\end{equation}

where $L = q \; c_L \; S$, $D = q \; c_D \; S$ with $q = \frac{\rho}{2}\; V^2$ and $V=\sqrt{u^2+w^2}$.

\subsection{Reward Function}
\label{sec:reward-function}

As shown in Fig. \ref{fig:agent_env_system}, the agent interacts with his environment by, at each time step, picking an action $a_t$ and receiving an observation $o_t$ and a reward $r_t$. In RL, the reward function is the only means to give the agent information about what he should or should not do.

The reward function in this work is as follows:

\begin{equation}
\mathcal{R}(s,a): \mathcal{S} \mapsto \mathbb{R}: r_t =
\begin{cases}
-\Delta t & \text{if } s_{t+1} \neq s_{T+}\\
-\Delta t + \frac{d}{10} & \text{if } s_{t+1} = s_{T+}
\end{cases}
\label{eq:reward_function}
\end{equation}

In Fig.~\ref{tikz:2d_state_space_discretized}, all state transitions, where the agent crosses a thick line (i.e. leaves the admissible part of the state space), lead to termination of the episode. If the agent hits one of the black thick lines, he gets a final reward of $0$. Only passing the green line earns him a final reward of $\frac{d}{10}$.

where $s_{T+}$ is a terminal state that is also a target state. There are terminal states $s_{T-}$ where the agent is not meant to go. These terminal states are those where the agent touches the ground before reaching his target range. If the agent reaches his goal, he receives a final reward $r_T$ according to the second line of Eq. \ref{eq:reward_function}. 

It is possible to penalize undesired behavior of the agent by defining a large negative reward if, for example, the agent touches the ground before reaching the target distance or flies too far in the wrong direction. This is, however, not necessary, if the final reward is sufficiently high. The important thing here is, that reaching a target state must always yield a higher return than ending an episode on the ground from any state $s \in \mathcal{S}$.\footnote{This is true in the scenarios in chapter \ref{chapter6}. In scenario 1, for example, the agent has to cover a horizontal distance of 500m. This takes him between 19.7s (c.f. table \ref{tab:2d_flight_data_500m}) and approx. 40s and earns him a time penalty of up to $-40$. With a final reward of $\frac{d}{10}=+50$, the resulting total reward is still positive and therefore sufficiently high for the agent to find the goal.}

\subsection{Discretization of the State- and Action Space}
\label{sec:disc2d}
The state space is discretized with a rectangular grid, i.e. it is replaced by a set of points $s_i=[x_k,z_l,u_m,w_n]^\top$ evenly distributed across all dimensions.

\begin{figure}
	\centering
	\tikzsetnextfilename{stategrid}
	\begin{tikzpicture}[scale=0.8]
		\foreach \col in {-1,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}{
			\draw[color=gray, line width=0.3pt] (\col,-1) -- +(0,9);
		}
		\foreach \row in {-1,0,1,2,3,4,5,6,7,8}{
			\draw[color=gray, line width=0.3pt] (-1,\row) -- +(17,0);
		}
		\draw[draw=black, line width=3pt] (0,0) rectangle (15,7);

		\foreach \i in {0,1,2,3,4}{
			\node at (\i-0.5,-1.5) {$x_\i$};
		}
		\node at (15.5,-1.5) {$x_{n_x}$};
		\foreach \j in {0,1,2,3}{
			\node at (-1.5,\j-0.5) {$z_\j$};
		}
		\node at (-1.5,7.5) {$z_{n_z}$};
		\draw (3.5,2.5) -- (2.75,10);
		\draw (3.5,2.5) -- (8.25,10);
		\draw (3.5,2.5) -- (2.75,14);
		\draw (3.5,2.5) -- (8.25,14);
		\draw [color=gray, fill=white] (2.75,10) rectangle (8.25,14);
		\foreach \vcol in {1,2,3,4,5,6,7}{
			\draw[color=gray, line width=0.3pt] (2.75+\vcol*0.6875,10) -- +(0,4);
		}
		\foreach \vrow in {1,2,3,4,5,6,7}{
			\draw[color=gray, line width=0.3pt] (2.75,10+\vrow*0.5) -- +(8*0.6875,0);
		}
%		\foreach \u in {-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5}{
%			\foreach \w in {-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5}{
%				\draw[draw=gray, fill=gray] (5.5+\u*0.6875,12+\w*0.5) circle (0.03cm);
%			}
%		}
		\foreach \k in {0,1,2}{
			\node at (3.09375+\k*0.6875,14.3) {$u_\k$};
		}
		\node at (3.09375+7*0.6875,14.3) {$u_{n_u}$};
		\foreach \l in {0,1,2}{
			\node at (2.2,10.25+\l*0.5) {$w_\l$};
		}
		\node at (2.2,13.75) {$w_{n_w}$};
		\draw[<->] (0,-2) -- node[above] {$d_T$} (15,-2);
		\draw[line width = 3pt, color = green] (15,0) -- (15,7);
		\draw[->,line width=1pt] (5.5,12) -- +(4.5,0) node [right] {${}_g u_K$};
		\draw[->,line width=1pt] (5.5,12) -- +(0,3.5) node [above] {$-{}_g w_K$};
		\draw[->,line width=1pt] (0,0) -- +(17,0) node [right] {$x_g$};
		\draw[->,line width=1pt] (0,0) -- +(0,9) node [above] {$-z_g$};
		\fill[green] (5.5,12) circle (3.5pt) ;
		\node[rotate=-80, xshift=-.2cm, yshift=.05cm, fill=none] (glider) at (0,7) {\includegraphics[ width=1.5cm, keepaspectratio]{./src/pics/tikz/simpleGlider.pdf}};
	\end{tikzpicture}
	\caption{The discretized state space in all 2D scenarios. }
	\label{tikz:2d_state_space_discretized}
\end{figure}

Fig. \ref{tikz:2d_state_space_discretized} illustrates the discretization of the state space. The agent starts its flight at the top left corner (at the glider symbol) with zero velocity (hence the green dot on the speed grid). For the grid cell at $x_4$ and $z_3$, the grid for the speed vector is drawn. All of the cells in the position grid contain a speed grid, like the small one in Fig. \ref{tikz:2d_state_space_discretized}.

In the 2D-scenario, the action space is one-dimensional, with the only action being the angle of attack $\alpha$. For the policy improvement step, a finite number of evenly spaced actions is sampled from the infinite set of possible actions between $\alpha_{min} = 0$ and $\alpha_{max}=0.2$. Assuming that the mapping from actions to returns is continuous, the action that yields the maximum expected return out of the sampled actions is an approximation of the true greedy action.

By discretization, the continuous trajectory optimization problem is made time- and space-discrete. For the purpose of keeping calculation time low, discretization should be as coarse as possible without affecting the result too much. A sample time $\Delta t$ of $1~\text{s}$ is used for all scenarios. With a horizontal speed of $15-25 ~\frac{\text{m}}{\text{s}}$, the agent covers about $15-25~\text{m}$ within one time step. The horizontal separation of grid points should be about $10$m. This way, the agent can skip one grid cell, if its velocity is sufficient. Grid cells, that are closer to the goal, usually have a higher state value. With the possibility to skip a cell, the agent is thus encouraged to fly at higher speeds. For the different scenarios, this yields the grid resolutions shown in table \ref{tab:grids}. In theory, more horizontal grid layers can yield a better solution. But because the tabular VI data is used to train a neural network, which "smoothes" out the given data, more than $52$ layers (i.e. 2m between cells vertically) have proven not to improve the result significantly.

\begin{table}
	\begin{center}
		\begin{tabular}{r|c c c}
			distance & $500~\text{m}$ & $1000~\text{m}$ & $2000~\text{m}$ \\ \hline
			$n_x$ & 52 & 102 & 202 \\
			$n_z$ & 52 & 52 & 52\\
			$n_u$ & 8 & 8 & 8 \\
			$n_w$ & 8 & 8 & 8
		\end{tabular}
	\end{center}
	\caption{Grid parameters for trajectory optimization}
	\label{tab:grids}
\end{table}

\section{3D Environment}

In the 3D scenario, the agent is put in a rectangular 3D state space and has to find a goal. Like in 2D, the goal is to reach the far end of the state space from his initial state. To make things more interesting, the direction the agent is flying in the beginning varies. In 3D, the agent can change the flying direction by performing a half loop, a horizontal curve, or a combination of both. What is optimal depends on the specific case.

In this three-dimensional environment, the state vector consists of three cartesian coordinates $\boldsymbol{{}_g r}=[x,y,z]^\top$ and three cartesian velocities $\boldsymbol{{}_g v_K}=[{}_g u_K,{}_g v_K,{}_g w_K]^\top$. The goal is for the agent to reach a given distance in minimal time. The controls are the angle of attack $\alpha$ and the bank angle $\mu$. The angle of attack works like in a 2D environment, whereas the bank angle lets the agent change his heading in order to reach the goal.

\subsection{Discretization of the state and action space}
\label{sec:disc3d}

Discretization is done like in the 2D scenario, which results in cuboids for the position and velocity.