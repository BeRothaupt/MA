% !TeX root = ../../../Main.tex
\chapter{Trajectory Optimization with Policy Iteration}

\section{2D Environment}

In the context of this work, the aircraft is treated as a mass point. Rotational dynamics are neglected. This is sufficient for trajectory optimization algorithms. See \cite{Fichter2009} for more details. 


In this scenario, the glider moves in the geodetic vertical plane. Its control is the angle of attack $\alpha$. When $\alpha$ is increased, the lift $L$ increases. This results in an increase of $\gamma$, as can be seen in equation \ref{eq:dotgamma}. This way the agent can control its velocity vector and therefore its position. All other parameters of the glider are shown in appendix \ref{appendix_A}. Any 3D-Scenario where the horizontal trajectory is restricted by obstacles or the optimal ground trace is obvious can be regarded as a 2D problem. The only control variable to be optimized is then the angle of attack or an equivalent quantity (e.g. vertical acceleration).

\subsection{Policy Representation}

As the Policy Iteration algorithm is running on a rectangular grid and the states that are updated do not change, a table that stores the greedy action for every state is sufficient for the Policy Iteration algorithm. Once the optimal policy is found, a neural network is trained to approximate the optimal action for each of the states on the grid. This policy is then used to do TRPO.

\subsection{Equations of Motion}

The physical state of the glider in the geodetic vertical plane consists of the position $\boldsymbol{{}_g r}=[x,z]^T$ and velocity $\boldsymbol{{}_g v}=[{}_g u_K,{}_g w_K]^T$. Apart from the geodetic reference frame (index g), a body fixed frame (index f), a trajectory fixed frame (index k) and an aerodynamic frame (index a) are used to describe the glider dynamics. In the vertical plane, coordinates can be transformed between the frames by a rotation around the y-axis. Figure \ref{fig:coords2d} illustrates the transformation angles that connect the reference frames. 

In the vertical plane, there are 4 state variables, $x$, $z$, $u=\dot{x}$ and $w=\dot{z}$, each with respect to a geodetic reference frame that has its origin at a point on the earth surface. The aerodynamic forces are often expressed in an aerodynamic reference frame. The coordinate transformations $u$ and $w$ can also be expressed by $V$ and $\phi_K$.

The dynamics in the vertical plane are given by the following equations. They are taken from \cite{Fichter2009}. 

\begin{equation}
\dot{x} = V \; cos\phi_K
\end{equation}

\begin{equation}
\dot{z} = - V \; sin\phi_K
\end{equation}

\begin{equation}
\dot{V} = -\frac{D + g \; cos\phi_K}{m}
\end{equation}

\begin{equation}
\dot{\phi_K} = \frac{L}{V \; m} - \frac{g \; cos{\phi_K}}{V} 
\label{eq:dotgamma}
\end{equation}

where $L = q \; c_L \; S$, $D = q \; c_D \; S = \frac{\rho}{2} \; V^2 \; c_d \; S$ and $V=\sqrt{u^2+w^2}$.

%\subsection{Maximum Range}
%
%An airplane with no propulsion system moving through calm air reaches its maximum range when flying at the maximum ratio of lift to drag. Lift and drag can be expressed as the product of dynamic pressure $q$, a reference area $S$ and a dimensionless coefficient $c_L$ or $c_D$:
%
%\begin{equation}
%L = q \; S \; c_L = \frac{\rho_K}{2} v^2 \; S \; c_L
%\end{equation}  
%
%\begin{equation}
%D = q \; S \; c_D
%\end{equation}
%
%\begin{equation}
%\frac{L}{D} = \frac{c_L}{c_D}
%\end{equation}
%
%The optimal lift to drag ratio is also the optimal ratio of $c_L$ and $c_D$:
%
%\begin{equation}
%\left(\frac{L}{D}\right)_{max} = \left(\frac{c_L}{c_D}\right)_{max}
%\end{equation}
%
%With a symmetric drag polar model, this yields:
%
%\begin{equation}
%c_{L,opt}=c_L\left(\left(\frac{c_L}{c_D}\right)_{opt}\right) = \sqrt{\frac{c_{D0}}{k}}=\sqrt{c_{D0}\;\pi\;\Lambda\;e}
%\end{equation}
%
%and
%
%\begin{equation}
%\alpha_{opt} \approx \frac{\Lambda+2}{2\;\pi\;\Lambda}\cdot c_{L,opt}
%\end{equation}
%
%A policy that is trained to return $\alpha_{opt}$ for every state can be used as a starting point for policy optimization with any policy optimization algorithm.
%
%\subsection{Following the Line of Sight Towards the Target}
%
%At each state in the state space, the line of sight from the agent to the target can be expressed by a unit vector pointing from the agent towards the target:
%
%\begin{equation}
%\vec{e}_{los} = \frac{\vec{r}_{target}-\vec{r}_{agent}}{|\vec{r}_{target}-\vec{r}_{agent}|}
%\end{equation}
%
%The angle $\phi_{los}\in [-\pi,\pi]$ between the horizontal axis and $\vec{e}_{los}$ can be expressed by the coordinates of the target and the agent.
%
%\begin{equation}
%\phi_{los}=\text{atan2}(d\Delta z,\Delta x) =\text{atan2}(z_{target}-z_{agent},x_{target}-x_{agent})
%\end{equation}
%
%$\phi_{los}$ is positive if the agent is below the target.
%
%The velocity vector $\vec{v}=[u,w]^T$ represents the direction in which the glider is moving. Like $\phi_{los}$, the flight path angle can be calculated with the atan2-function.
%
%\begin{equation}
%\phi_K = \text{atan2}(u,w)
%\end{equation}

\subsection{Discretization of the State- and Action Space}
\label{sec:disc2d}
The state space is discretized with a rectangular grid, i.e. it is replaced by a set of points $s_n=[x_k,z_l,u_m,w_n]$ evenly distributed across all dimensions.

\begin{figure}
	\centering
	\tikzsetnextfilename{stategrid}
	\begin{tikzpicture}
		\draw[draw=black] (0,0) rectangle (15,7);
		\foreach \x in {-0.5,0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5,10.5,11.5,12.5,13.5,14.5,15.5}{
			\foreach \z in {-0.5,0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5}{
				\draw[draw=gray, fill=gray] (\x,\z) circle (0.03cm);
			}
		}
		\foreach \i in {0,1,2,3,4}{
			\node at (\i-0.5,-1) {$x_\i$};
		}
		\node at (15.5,-1) {$x_K$};
		\foreach \j in {0,1,2,3}{
			\node at (-1,\j-0.5) {$z_\j$};
		}
		\node at (-1,7.5) {$z_L$};
		\draw (3.5,2.5) -- (2.75,8.25);
		\draw (3.5,2.5) -- (8.25,8.25);
		\draw (3.5,2.5) -- (2.75,11.75);
		\draw (3.5,2.5) -- (8.25,11.75);
		\draw [color=gray, fill=white] (2.75,8.25) rectangle (8.25,11.75);
		\foreach \u in {-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5}{
			\foreach \w in {-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5}{
				\draw[draw=black, fill=black] (5.5+\u*0.6875,10+\w*0.4375) circle (0.03cm);
			}
		}
		\foreach \k in {0,1,2}{
			\node at (3.09375+\k*0.6875,12) {$u_\k$};
		}
		\node at (3.09375+7*0.6875,12) {$u_M$};
		\foreach \l in {0,1,2}{
			\node at (2.2,10-3.5*0.435+\l*0.4375) {$w_\l$};
		}
		\node at (2.2,10+3.5*0.4375) {$w_N$};
		\draw[<->] (0,-1.5) -- node[below] {$d_T$} (15,-1.5);
		\draw[->] (5.5,10) -- +(2.5*0.6875,1.5*0.4375);
		\draw[->] (0,0) -- (3.5,2.5);
%		\node at (3.5,2.5) {Text} ;
	\end{tikzpicture}
	\caption{The discretized state space. For $x_4$ and $z_3$, the grid for the speed vector is drawn.}
	\label{tikz:2d_state_space_discretized}
\end{figure}

In the 2D-scenario, the action space is one-dimensional, with the only action being the angle of attack $\alpha$. For the policy improvement step, a finite number of evenly spaced actions is sampled from the infinite set of possible actions between $\alpha_{min} = 0$ and $\alpha_{max}=0.2$. Assuming that the mapping from actions to returns is continuous, the action that yields the maximum expected return out of the sampled actions is an approximation of the true greedy action.

By discretization, the continuous trajectory optimization problem is made time- and space-discrete. For the purpose of keeping calculation time low, a sample time $\Delta t$ of $1s$ is used for the 1000m and 2000m flights. With a speed of $15-25 \frac{\text{m}}{\text{s}}$, the agent covers about $15-25\text{m}$ within one time step. In the scenarios, where the agent has to cover 500m, the sample-time is $0.5s$. The grid points should be separated by approximately that distance in order to make tabular solution methods feasible. For the different scenarios, this yields the grid resolutions shown in table \ref{tab:grids}.
\begin{table}
	\begin{center}
		\begin{tabular}{r|c c c}
			distance & $500m$ & $1000m$ & $2000m$ \\ \hline
			$n_x$ & 52 & 52 & 102 \\
			$n_z$ & 42 & 22 & 22\\
			$n_u$ & 8 & 8 & 8 \\
			$n_w$ & 8 & 8 & 8
		\end{tabular}
		\caption{Grid parameters for trajectory optimization}
		\label{tab:grids}
	\end{center}
\end{table}

\section{3D Environment}

todo

\subsection{Discretization of the state and action space}
\label{sec:disc3d}
todo