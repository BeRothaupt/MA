% Encoding: utf8 without BOM

@BOOK{Fichter2009,
  title = {Flugmechanik},
  publisher = {Shaker Verlag},
  year = {2009},
  author = {Walter Fichter and Werner Grimm}
}

@book{Zuern2017,
	author = {Markus Zuern},
	year = {2017},
	title = {Autonomous Soaring through Unknown Atmosphere, Master Thesis},
	publisher = {Institute for Flight Mechanics and Control, Stuttgart University}
}

@article{Notter2018,
	author = {Stefan Notter and Markus Zuern and Pascal Gross and Walter Fichter},
	year   = {2018},
	title  = {Reinforced Learning to Cross-Country Soar in the Vertical Plane of Motion}
}

@article{DBLP:journals/corr/KingmaB14,
	author    = {Diederik P. Kingma and
	Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	journal   = {CoRR},
	volume    = {abs/1412.6980},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.6980},
	archivePrefix = {arXiv},
	eprint    = {1412.6980},
	timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{SuttonBarto2018,
	author    =  {Richard S. Sutton and Andrew G. Barto},
	title     =  {Reinforcement Learning - An Introduction},
	year      =  {2018},
	publisher =  {The MIT Press}}

@website{Silver2015,
	author    =  {David Silver},
	title     =  {UCL Course on Reinforcement Learning},
	year      =  {2015}}

@article{Bellman1957,
	author    =  {Richard E. Bellman},
	title     =  {A Markovian Decision Process},
	year      =  {1957}}

@article{Bellman1954,
	author    =  {Richard E. Bellman},
	title     =  {The Theory of Dynamic Programming},
	year      =  {1954}}

@article{Bellman1962,
	author    =  {Richard E. Bellman},
	title     =  {Applied Dynamic Programming},
	year      =  {1962}}

@book {Powell2007ADP,
	author = {Powell, Warren B.},
	title = {Approximate dynamic programming: solving the curses of dimensionality},
	series = {Wiley series in probability and statistics},
	address = {Hoboken, NJ},
	publisher = {Wiley-Interscience},
	year = {2007},
	pages = {XVI, 460  Seiten},
	ISBN = {978-0-470-17155-4},
	language = {Englisch},
	keywords = {Dynamic programming},
	URL = {http://swbplus.bsz-bw.de/bsz275179400cov.htm},
	URL = {http://www.gbv.de/dms/ilmenau/toc/527185191.PDF},
	note = {UB Vaihingen}
}

@Book{Kriesel2007NeuralNetworks, 
	author = { David Kriesel }, 
	title =  { A Brief Introduction to Neural Networks},
	year =   { 2007 }, 
	url =   { available at http://www.dkriesel.com } 
}

@misc{Schulman2016,
	author = { Pieter Abbeel and John Schulmann},
	title = { Deep Reinforcement Learning through Policy Optimization },
	url = {https://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf},
	year = {2016}
}

@online{rllab2018,
	title = { rllab-website },
	url = {https://rllab.readthedocs.io/en/latest/},
	year = {2018},
	note = {visited on August 17, 2018}
}

@book{grimm2004,
	title ={Bahnoptimierung f√ºr Luft- und Raumfahrzeuge},
	author = {Werner Grimm},
	year = {2004},
	note = {Skriptum zur Vorlesung}
}

@article{DBLP:journals/corr/SchulmanLMJA15,
	author    = {John Schulman and
	Sergey Levine and
	Philipp Moritz and
	Michael I. Jordan and
	Pieter Abbeel},
	title     = {Trust Region Policy Optimization},
	journal   = {CoRR},
	volume    = {abs/1502.05477},
	year      = {2015},
	url       = {http://arxiv.org/abs/1502.05477},
	archivePrefix = {arXiv},
	eprint    = {1502.05477},
	timestamp = {Mon, 13 Aug 2018 16:48:08 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanLMJA15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ConvDiscDP, 
	author={D. Bertsekas}, 
	journal={IEEE Transactions on Automatic Control}, 
	title={Convergence of discretization procedures in dynamic programming}, 
	year={1975}, 
	volume={20}, 
	number={3}, 
	pages={415-419}, 
	keywords={Dynamic programming;Nonlinear systems, stochastic discrete-time;Optimal stochastic control;Stochastic optimal control;Convergence;Dynamic programming;Stochastic processes;Optimal control;Cost function;Heuristic algorithms;Grid computing;Concrete;Veins;Probability distribution}, 
	doi={10.1109/TAC.1975.1100984}, 
	ISSN={0018-9286}, 
	month={June},}

@article{Tsitsiklis2003,
	author = {Tsitsiklis, John N.},
	title = {On the Convergence of Optimistic Policy Iteration},
	journal = {J. Mach. Learn. Res.},
	issue_date = {3/1/2003},
	volume = {3},
	month = mar,
	year = {2003},
	issn = {1532-4435},
	pages = {59--72},
	numpages = {14},
	url = {https://doi.org/10.1162/153244303768966102},
	doi = {10.1162/153244303768966102},
	acmid = {944922},
	publisher = {JMLR.org},
	keywords = {Markov decision problem, Monte Carlo, Stochastic approximation, dynamic programming, reinforcement learning, temporal differences},
} 

@article{Hordijk_PI_conv_1987,
	ISSN = {0364765X, 15265471},
	URL = {http://www.jstor.org/stable/3689678},
	abstract = {We study the convergence of policy iteration for the undiscounted, finite state, discrete time Markov decision problem with compact action space and unichain transition structure. Using a "Newton Method type" representation for policy iteration, we establish the existence of a solution to the optimality equation. We show that to find an average optimal policy, it is sufficient to solve the optimality equation on the recurrent set of the maximizing policy. Under the additional assumption of a unique maximizing policy at each stage of the policy iteration procedure, we show that the iterates are convergent and the resulting policy is Blackwell optimal.},
	author = {Arie Hordijk and Martin L. Puterman},
	journal = {Mathematics of Operations Research},
	number = {1},
	pages = {163--176},
	publisher = {INFORMS},
	title = {On the Convergence of Policy Iteration in Finite State Undiscounted Markov Decision Processes: The Unichain Case},
	volume = {12},
	year = {1987}
}

@article{cavazos1998,
	author = {Cavazos-Cadena, Rolando},
	year = {1998},
	month = {04},
	pages = {},
	title = {Undiscounted Value Iteration in Stable Markov Decision Chains with Bounded Rewards}
}

@article{Wirth2015MeteorologicalPP,
	title={Meteorological path planning using dynamic programming for a solar-powered UAV},
	author={Lukas Wirth and Philipp Oettershagen and Jacques Ambuhl and Roland Siegwart},
	journal={2015 IEEE Aerospace Conference},
	year={2015},
	pages={1-11}
}

@article {ReddyE4877,
	author = {Reddy, Gautam and Celani, Antonio and Sejnowski, Terrence J. and Vergassola, Massimo},
	title = {Learning to soar in turbulent environments},
	volume = {113},
	number = {33},
	pages = {E4877--E4884},
	year = {2016},
	doi = {10.1073/pnas.1606075113},
	publisher = {National Academy of Sciences},
	abstract = {Thermals are ascending currents that typically extend from the ground up to the base of the clouds. Birds and gliders piggyback thermals to fly with a reduced expenditure of energy, for example, during migration, and to extend their flying range. Flow in the thermals is highly turbulent, which poses the challenge of the orientation in strongly fluctuating environments. We combine numerical simulations of atmospheric flow with reinforcement learning methods to identify strategies of navigation that can cope with and even exploit turbulent fluctuations. Specifically, we show how the strategies evolve as the level of turbulent fluctuations increase, and we identify those sensorimotor cues that are effective at directing turbulent navigation.Birds and gliders exploit warm, rising atmospheric currents (thermals) to reach heights comparable to low-lying clouds with a reduced expenditure of energy. This strategy of flight (thermal soaring) is frequently used by migratory birds. Soaring provides a remarkable instance of complex decision making in biology and requires a long-term strategy to effectively use the ascending thermals. Furthermore, the problem is technologically relevant to extend the flying range of autonomous gliders. Thermal soaring is commonly observed in the atmospheric convective boundary layer on warm, sunny days. The formation of thermals unavoidably generates strong turbulent fluctuations, which constitute an essential element of soaring. Here, we approach soaring flight as a problem of learning to navigate complex, highly fluctuating turbulent environments. We simulate the atmospheric boundary layer by numerical models of turbulent convective flow and combine them with model-free, experience-based, reinforcement learning algorithms to train the gliders. For the learned policies in the regimes of moderate and strong turbulence levels, the glider adopts an increasingly conservative policy as turbulence levels increase, quantifying the degree of risk affordable in turbulent environments. Reinforcement learning uncovers those sensorimotor cues that permit effective control over soaring in turbulent environments.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/113/33/E4877},
	eprint = {http://www.pnas.org/content/113/33/E4877.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}
