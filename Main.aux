\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Erkl\IeC {\"a}rung}{iii}{chapter*.1}}
\@writefile{toc}{\contentsline {chapter}{Abstract/Kurzfassung}{v}{chapter*.1}}
\citation{Notter2018}
\citation{Schulman2016}
\citation{SuttonBarto2018}
\citation{SuttonBarto2018}
\citation{Zuern2017}
\citation{Zuern2017}
\citation{Notter2018}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xi}{chapter*.5}}
\@writefile{toc}{\contentsline {chapter}{Nomenklatur}{xiii}{chapter*.7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{ReddyE4877}
\citation{Zuern2017}
\citation{Zuern2017}
\citation{Notter2018}
\citation{Zuern2017}
\citation{Bellman1954}
\citation{Bellman1962}
\citation{Powell2007ADP}
\citation{Wirth2015MeteorologicalPP}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Solution Methods for Markov Decision Processes}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter2}{{2}{5}{Solution Methods for Markov Decision Processes}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Process}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Principle of Optimality}{5}{section.2.2}}
\newlabel{sec:optimality}{{2.2}{5}{The Principle of Optimality}{section.2.2}{}}
\citation{Notter2018}
\citation{Notter2018}
\citation{SuttonBarto2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The Agent - Environment System}{6}{section.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Model Based and Model Free Learning}{6}{section.2.4}}
\newlabel{sec:modelbasedmodelfree}{{2.4}{6}{Model Based and Model Free Learning}{section.2.4}{}}
\citation{SuttonBarto2018}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces The Agent-Environment-System~\cite  {Notter2018}}}{7}{figure.2.1}}
\newlabel{fig:agent_env_system}{{\relax 2.1}{7}{The Agent-Environment-System~\cite {Notter2018}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Agent}{8}{section.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Return}{8}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Policy}{9}{section.2.7}}
\newlabel{sec:policy}{{2.7}{9}{Policy}{section.2.7}{}}
\newlabel{eq:transprobpi}{{\relax 2.8}{9}{Policy}{equation.2.7.8}{}}
\newlabel{eq:rewardpi}{{\relax 2.9}{9}{Policy}{equation.2.7.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Value Functions}{10}{section.2.8}}
\newlabel{sec:value-function}{{2.8}{10}{Value Functions}{section.2.8}{}}
\newlabel{eq:state_value_fun}{{\relax 2.10}{10}{Value Functions}{equation.2.8.10}{}}
\newlabel{eq:action_value_fun}{{\relax 2.11}{10}{Value Functions}{equation.2.8.11}{}}
\newlabel{eq:state_value_function_with_q}{{\relax 2.12}{10}{Value Functions}{equation.2.8.12}{}}
\newlabel{eq:action_value_function_with_v}{{\relax 2.13}{10}{Value Functions}{equation.2.8.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}The Bellman Equation}{10}{section.2.9}}
\newlabel{sec:Bellman_Equation}{{2.9}{10}{The Bellman Equation}{section.2.9}{}}
\citation{SuttonBarto2018}
\newlabel{eq:bellman_exp_eq_V_Q}{{\relax 2.15}{11}{The Bellman Equation}{equation.2.9.15}{}}
\newlabel{eq:bellman_exp_eq_V_determinisic}{{\relax 2.16}{11}{The Bellman Equation}{equation.2.9.16}{}}
\newlabel{eq:bellman_optimality_equation_v_with_q}{{\relax 2.17}{11}{The Bellman Equation}{equation.2.9.17}{}}
\newlabel{eq:bellman_optimality_equation_stochastic}{{\relax 2.21}{11}{The Bellman Equation}{equation.2.9.21}{}}
\newlabel{eq:bellman_optimality_equation_deterministic}{{\relax 2.22}{11}{The Bellman Equation}{equation.2.9.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.10}Solving an MDP}{11}{section.2.10}}
\citation{Zuern2017}
\citation{Schulman2016}
\citation{Schulman2016}
\@writefile{toc}{\contentsline {section}{\numberline {2.11}Dynamic Programming}{12}{section.2.11}}
\newlabel{chapter3}{{2.11}{12}{Dynamic Programming}{section.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.2}{\ignorespaces MDP solution methods, adopted from~\cite  {Schulman2016}]}}{12}{figure.2.2}}
\newlabel{fig:RLmethods}{{\relax 2.2}{12}{MDP solution methods, adopted from~\cite {Schulman2016}]}{figure.2.2}{}}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.1}The Curses of Dimensionality}{13}{subsection.2.11.1}}
\newlabel{sec:curses}{{2.11.1}{13}{The Curses of Dimensionality}{subsection.2.11.1}{}}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{grimm2004}
\citation{Silver2015}
\citation{SuttonBarto2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.2}Approximate Dynamic Programming}{14}{subsection.2.11.2}}
\newlabel{eq:finance_resources}{{\relax 2.23}{14}{Approximate Dynamic Programming}{equation.2.11.23}{}}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{Powell2007ADP}
\citation{SuttonBarto2018}
\citation{SuttonBarto2018}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.3}{\ignorespaces Dynamic programming as a reinforcement learning method (taken from \cite  {SuttonBarto2018})}}{15}{figure.2.3}}
\newlabel{fig:dp_in_rl}{{\relax 2.3}{15}{Dynamic programming as a reinforcement learning method (taken from \cite {SuttonBarto2018})}{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.3}Types of Dynamic Programming Algorithms}{16}{subsection.2.11.3}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Policy Evaluation}{16}{section*.12}}
\newlabel{subsection:policy_evaluation}{{2.11.3}{16}{Policy Evaluation}{section*.12}{}}
\newlabel{eq:bellman_exp}{{\relax 2.26}{16}{Policy Evaluation}{equation.2.11.26}{}}
\newlabel{eq:bellman_exp_discrete_policy}{{\relax 2.27}{16}{Policy Evaluation}{equation.2.11.27}{}}
\newlabel{eq:bellman_exp_update}{{\relax 2.28}{16}{Policy Evaluation}{equation.2.11.28}{}}
\newlabel{eq:bellman_exp_update_bootstrapped}{{\relax 2.30}{16}{Policy Evaluation}{equation.2.11.30}{}}
\newlabel{eq:bellman_exp_update_discrete}{{\relax 2.31}{16}{Policy Evaluation}{equation.2.11.31}{}}
\newlabel{eq:pe_stopping_criterion}{{\relax 2.32}{17}{Policy Evaluation}{equation.2.11.32}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Iterative policy evaluation}}{17}{algorithm.1}}
\newlabel{algo:pe}{{1}{17}{Policy Evaluation}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Policy Iteration}{17}{section*.13}}
\newlabel{sec:PI}{{2.11.3}{17}{Policy Iteration}{section*.13}{}}
\citation{SuttonBarto2018}
\citation{SuttonBarto2018}
\citation{SuttonBarto2018}
\newlabel{eq:get_greedy_action}{{\relax 2.33}{18}{Policy Iteration}{equation.2.11.33}{}}
\newlabel{eq:pi_scheme}{{2.11.3}{18}{Policy Iteration}{equation.2.11.36}{}}
\citation{Silver2015}
\citation{Silver2015}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.4}{\ignorespaces The policy iteration algorithm (adopted from~\cite  [section~4.6]{SuttonBarto2018}}}{19}{figure.2.4}}
\newlabel{fig:PI_triangle}{{\relax 2.4}{19}{The policy iteration algorithm (adopted from~\cite [section~4.6]{SuttonBarto2018}}{figure.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Value Iteration}{19}{section*.14}}
\newlabel{subsection:VI}{{2.11.3}{19}{Value Iteration}{section*.14}{}}
\newlabel{eq:value_iteration_update}{{\relax 2.37}{19}{Value Iteration}{equation.2.11.37}{}}
\newlabel{eq:vi_scheme}{{2.11.3}{19}{Value Iteration}{equation.2.11.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11.4}The Contraction Mapping Theorem}{20}{subsection.2.11.4}}
\newlabel{sec:contraction_mappings}{{2.11.4}{20}{The Contraction Mapping Theorem}{subsection.2.11.4}{}}
\newlabel{eq:bellman_operator_discounted}{{\relax 2.39}{20}{Discounted MDPs}{equation.2.11.39}{}}
\citation{Tsitsiklis2003}
\citation{Hordijk_PI_conv_1987}
\citation{cavazos1998}
\citation{ConvDiscDP}
\citation{Powell2007ADP}
\newlabel{eq:4.15}{{\relax 2.42}{21}{Discounted MDPs}{equation.2.11.42}{}}
\newlabel{eq:4.16}{{\relax 2.43}{21}{Discounted MDPs}{equation.2.11.43}{}}
\newlabel{eq:4.17}{{\relax 2.44}{21}{Discounted MDPs}{equation.2.11.44}{}}
\newlabel{eq:4.18}{{\relax 2.45}{21}{Discounted MDPs}{equation.2.11.45}{}}
\newlabel{eq:4.19}{{\relax 2.46}{21}{Discounted MDPs}{equation.2.11.46}{}}
\newlabel{eq:bellman_undiscounted}{{\relax 2.47}{21}{Undiscounted MDPs}{equation.2.11.47}{}}
\newlabel{eq:n-step-contraction}{{\relax 2.48}{21}{Undiscounted MDPs}{equation.2.11.48}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.12}Exploiting the Problem Structure}{21}{section.2.12}}
\newlabel{sec:prob_structure}{{2.12}{21}{Exploiting the Problem Structure}{section.2.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Generalized policy iteration}}{23}{algorithm.2}}
\newlabel{algo:gpi}{{2}{23}{Policy Iteration}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Value iteration}}{24}{algorithm.3}}
\newlabel{algo:vi}{{3}{24}{Value Iteration}{algorithm.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Function Approximation}{25}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter4}{{3}{25}{Function Approximation}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Tables}{25}{section.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Nearest Neighbor}{25}{section*.17}}
\citation{Kriesel2007NeuralNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Linear Interpolation}{26}{section*.18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Artificial Neural Networks}{26}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces A neuron in an artificial neural network}}{26}{figure.3.1}}
\newlabel{tikz:neuron}{{\relax 3.1}{26}{A neuron in an artificial neural network}{figure.3.1}{}}
\citation{Zuern2017}
\citation{Zuern2017}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces Activation functions (adopted from \cite  {Zuern2017})}}{27}{figure.3.2}}
\newlabel{fig:activation_functions}{{\relax 3.2}{27}{Activation functions (adopted from \cite {Zuern2017})}{figure.3.2}{}}
\citation{Zuern2017}
\citation{Zuern2017}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces The policy MLP~\cite  {Zuern2017}}}{28}{figure.3.3}}
\newlabel{tikz:MLP}{{\relax 3.3}{28}{The policy MLP~\cite {Zuern2017}}{figure.3.3}{}}
\newlabel{eq:theta}{{\relax 3.3}{28}{Artificial Neural Networks}{equation.3.2.3}{}}
\newlabel{sec:input_standardization}{{3.2}{29}{Input Standardization}{section*.19}{}}
\newlabel{sec:weight_init}{{3.2}{30}{Weight initialization}{section*.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Supervised Learning}{30}{section.3.3}}
\newlabel{eq:lossfun}{{\relax 3.7}{30}{Supervised Learning}{equation.3.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Optimization Techniques}{31}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Gradient Descent}{31}{subsection.3.4.1}}
\newlabel{sec:grad_desc}{{3.4.1}{31}{Gradient Descent}{subsection.3.4.1}{}}
\newlabel{eq:gd_update}{{\relax 3.8}{31}{Gradient Descent}{equation.3.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Stochastic Gradient Descent}{31}{subsection.3.4.2}}
\newlabel{sec:sgd}{{3.4.2}{31}{Stochastic Gradient Descent}{subsection.3.4.2}{}}
\citation{DBLP:journals/corr/KingmaB14}
\newlabel{sgd_gradient}{{\relax 3.11}{32}{Stochastic Gradient Descent}{equation.3.4.11}{}}
\newlabel{sgd_update}{{\relax 3.12}{32}{Stochastic Gradient Descent}{equation.3.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}The Adam Algorithm}{32}{subsection.3.4.3}}
\newlabel{sec:adam}{{3.4.3}{32}{The Adam Algorithm}{subsection.3.4.3}{}}
\newlabel{eq:adam_update}{{\relax 3.13}{32}{The Adam Algorithm}{equation.3.4.13}{}}
\newlabel{eq:adam_hat_m}{{\relax 3.14}{32}{The Adam Algorithm}{equation.3.4.14}{}}
\newlabel{eq:adam_hat_v}{{\relax 3.15}{32}{The Adam Algorithm}{equation.3.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Overfitting}{33}{section.3.5}}
\newlabel{eq:overfitting_fitting}{{\relax 3.17}{33}{Overfitting}{equation.3.5.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.4}{\ignorespaces Overfitting in case of a high order polynomial and noisy data}}{34}{figure.3.4}}
\newlabel{tikz:overfitting}{{\relax 3.4}{34}{Overfitting in case of a high order polynomial and noisy data}{figure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.5}{\ignorespaces Stop criterion to avoid overfitting}}{35}{figure.3.5}}
\newlabel{tikz:stopoverfitting}{{\relax 3.5}{35}{Stop criterion to avoid overfitting}{figure.3.5}{}}
\newlabel{eq:losswithweights}{{\relax 3.18}{36}{Overfitting}{equation.3.5.18}{}}
\citation{Fichter2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Trajectory Optimization with Dynamic Programming}{37}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter5}{{4}{37}{Trajectory Optimization with Dynamic Programming}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}2D Environment}{37}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Policy Representation}{37}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Equations of Motion}{37}{subsection.4.1.2}}
\citation{Notter2018}
\citation{Notter2018}
\citation{Fichter2009}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.1}{\ignorespaces Reference frame transformation~\cite  {Notter2018}}}{38}{figure.4.1}}
\newlabel{fig:coords2d}{{\relax 4.1}{38}{Reference frame transformation~\cite {Notter2018}}{figure.4.1}{}}
\newlabel{eq:dotgamma}{{\relax 4.4}{39}{Equations of Motion}{equation.4.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Reward Function}{39}{subsection.4.1.3}}
\newlabel{sec:reward-function}{{4.1.3}{39}{Reward Function}{subsection.4.1.3}{}}
\newlabel{eq:reward_function}{{\relax 4.5}{39}{Reward Function}{equation.4.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Discretization of the State- and Action Space}{39}{subsection.4.1.4}}
\newlabel{sec:disc2d}{{4.1.4}{39}{Discretization of the State- and Action Space}{subsection.4.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.2}{\ignorespaces The discretized state space in all 2D scenarios. }}{40}{figure.4.2}}
\newlabel{tikz:2d_state_space_discretized}{{\relax 4.2}{40}{The discretized state space in all 2D scenarios}{figure.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 4.1}{\ignorespaces Grid parameters for trajectory optimization}}{42}{table.4.1}}
\newlabel{tab:grids}{{\relax 4.1}{42}{Grid parameters for trajectory optimization}{table.4.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Results}{43}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter6}{{5}{43}{Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}2D Dynamic Programming}{43}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Optimal Control Benchmark}{43}{subsection.5.1.1}}
\newlabel{sec:results_dp}{{5.1.1}{43}{Optimal Control Benchmark}{subsection.5.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.1}{\ignorespaces Some information about the investigated scenarios}}{43}{table.5.1}}
\newlabel{tab:scenario_data}{{\relax 5.1}{43}{Some information about the investigated scenarios}{table.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Scenario 1 - Coarse Discretization}{44}{section*.22}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.2}{\ignorespaces Scenario 1, comparison of synchronous algorithms on a coarse grid}}{45}{table.5.2}}
\newlabel{tab:2d_flight_data_500m}{{\relax 5.2}{45}{Scenario 1, comparison of synchronous algorithms on a coarse grid}{table.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.3}{\ignorespaces Scenario 1, comparison of asynchronous algorithms on a coarse grid}}{45}{table.5.3}}
\newlabel{tab:2d_flight_data_500m_async}{{\relax 5.3}{45}{Scenario 1, comparison of asynchronous algorithms on a coarse grid}{table.5.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.4}{\ignorespaces Scenario 2, performance of asynchronous VI on a fine grid}}{45}{table.5.4}}
\newlabel{tab:2d_flight_data_500m_async_fine_grid}{{\relax 5.4}{45}{Scenario 2, performance of asynchronous VI on a fine grid}{table.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Scenario 1 - Fine Discretization}{45}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.1}{\ignorespaces Results of synchronous value iteration and optimal control in scenario 1}}{46}{figure.5.1}}
\newlabel{tikz:vi500mcoarse}{{\relax 5.1}{46}{Results of synchronous value iteration and optimal control in scenario 1}{figure.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.5}{\ignorespaces Flight- and computation-times for OC and asynchronous VI in scenario 2}}{46}{table.5.5}}
\newlabel{tab:2d_flight_data_1000m}{{\relax 5.5}{46}{Flight- and computation-times for OC and asynchronous VI in scenario 2}{table.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.2}{\ignorespaces Results of value iteration and optimal control in scenario 1 with a fine grid}}{47}{figure.5.2}}
\newlabel{tikz:vi500m}{{\relax 5.2}{47}{Results of value iteration and optimal control in scenario 1 with a fine grid}{figure.5.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Scenario 2}{47}{section*.24}}
\citation{DBLP:journals/corr/SchulmanLMJA15}
\citation{Zuern2017}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.3}{\ignorespaces Results of value iteration and optimal control in scenario 2}}{48}{figure.5.3}}
\newlabel{tikz:vi1000m}{{\relax 5.3}{48}{Results of value iteration and optimal control in scenario 2}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Policy Initialization for TRPO}{48}{subsection.5.1.2}}
\newlabel{sec:dp_init_trpo}{{5.1.2}{48}{Policy Initialization for TRPO}{subsection.5.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.4}{\ignorespaces Average return per iteration with and without policy initialization}}{49}{figure.5.4}}
\newlabel{tikz:trpoglorotvi1000mAvg}{{\relax 5.4}{49}{Average return per iteration with and without policy initialization}{figure.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.5}{\ignorespaces Maximum return per iteration with and without policy initialization}}{49}{figure.5.5}}
\newlabel{tikz:trpoglorotvi1000mMax}{{\relax 5.5}{49}{Maximum return per iteration with and without policy initialization}{figure.5.5}{}}
\citation{Zuern2017}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.6}{\ignorespaces Flight times of agents with three different policy initialization techniques and different start states}}{50}{table.5.6}}
\newlabel{tab:vi_trpo_flighttimes}{{\relax 5.6}{50}{Flight times of agents with three different policy initialization techniques and different start states}{table.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.6}{\ignorespaces VI - trajectories from various initial states with zero velocity}}{51}{figure.5.6}}
\newlabel{tikz:vi_result_before_trpo}{{\relax 5.6}{51}{VI - trajectories from various initial states with zero velocity}{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.7}{\ignorespaces VI - trajectories from various initial states with ${}_g u_K = -20 \frac  {m}{s}$}}{51}{figure.5.7}}
\newlabel{tikz:vi_result_before_trpo_2}{{\relax 5.7}{51}{VI - trajectories from various initial states with ${}_g u_K = -20 \frac {m}{s}$}{figure.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.8}{\ignorespaces TRPO - trajectories from various initial states with zero velocity}}{52}{figure.5.8}}
\newlabel{tikz:trpo_result}{{\relax 5.8}{52}{TRPO - trajectories from various initial states with zero velocity}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.9}{\ignorespaces TRPO - trajectories from various initial states with ${}_g u_K = -20 \frac  {m}{s}$}}{52}{figure.5.9}}
\newlabel{tikz:trpo_result_2}{{\relax 5.9}{52}{TRPO - trajectories from various initial states with ${}_g u_K = -20 \frac {m}{s}$}{figure.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.10}{\ignorespaces VI and TRPO - trajectories from various initial states with zero velocity}}{53}{figure.5.10}}
\newlabel{tikz:vi_result_after_trpo}{{\relax 5.10}{53}{VI and TRPO - trajectories from various initial states with zero velocity}{figure.5.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.11}{\ignorespaces VI and TRPO - trajectories from various initial states with ${}_g u_K = -20 \frac  {m}{s}$}}{53}{figure.5.11}}
\newlabel{tikz:vi_result_after_trpo_2}{{\relax 5.11}{53}{VI and TRPO - trajectories from various initial states with ${}_g u_K = -20 \frac {m}{s}$}{figure.5.11}{}}
\citation{Zuern2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion}{55}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter7}{{6}{55}{Discussion}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{57}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Glider Parameters}{57}{section.A.1}}
\newlabel{appendix_A}{{A.1}{57}{Glider Parameters}{section.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax A.1}{\ignorespaces glider data}}{57}{table.A.1}}
\newlabel{tab:glider_data}{{\relax A.1}{57}{glider data}{table.A.1}{}}
\citation{rllab2018}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Computer Configuration and Implementation}{58}{section.A.2}}
\newlabel{appendix_B}{{A.2}{58}{Computer Configuration and Implementation}{section.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}Computer Configuration}{58}{subsection.A.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax A.2}{\ignorespaces computer specifications}}{58}{table.A.2}}
\newlabel{tab:pc_specs}{{\relax A.2}{58}{computer specifications}{table.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.2}Computational Subtleties of PI and VI}{59}{subsection.A.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Illustration of a One Step Lookahead}{60}{section.A.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax A.1}{\ignorespaces A one step lookahead in the 2D environment.}}{60}{figure.A.1}}
\newlabel{tikz:osl}{{\relax A.1}{60}{A one step lookahead in the 2D environment}{figure.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Optimistic Policy Iteration}{61}{section.A.4}}
\newlabel{appendix_D}{{A.4}{61}{Optimistic Policy Iteration}{section.A.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Optimistic Policy Iteration}}{61}{algorithm.4}}
\newlabel{algo:opi}{{4}{61}{Optimistic Policy Iteration}{algorithm.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Table Representation of the Value Function and Policy}{62}{section.A.5}}
\newlabel{appendix_C}{{A.5}{62}{Table Representation of the Value Function and Policy}{section.A.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax A.2}{\ignorespaces A four-dimensional table.}}{62}{figure.A.2}}
\newlabel{tikz:4d_table}{{\relax A.2}{62}{A four-dimensional table}{figure.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Scenario 1: Additional Results}{63}{section.A.6}}
\newlabel{appendixE}{{A.6}{63}{Scenario 1: Additional Results}{section.A.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax A.3}{\ignorespaces Results of synchronous generalized policy iteration and optimal control in scenario 1 with a coarse grid}}{63}{figure.A.3}}
\newlabel{tikz:gpi500mcoarse}{{\relax A.3}{63}{Results of synchronous generalized policy iteration and optimal control in scenario 1 with a coarse grid}{figure.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax A.4}{\ignorespaces Results of synchronous optimistic policy iteration and optimal control in scenario 1 with a coarse grid}}{64}{figure.A.4}}
\newlabel{tikz:opi500mcoarse}{{\relax A.4}{64}{Results of synchronous optimistic policy iteration and optimal control in scenario 1 with a coarse grid}{figure.A.4}{}}
\bibstyle{abbrvdin}
\bibdata{./src/bib/bibfile}
\bibcite{rllab2018}{1}
\bibcite{Schulman2016}{2}
\bibcite{Bellman1954}{3}
\bibcite{Bellman1962}{4}
\bibcite{ConvDiscDP}{5}
\bibcite{cavazos1998}{6}
\bibcite{Fichter2009}{7}
\bibcite{grimm2004}{8}
\bibcite{Hordijk_PI_conv_1987}{9}
\bibcite{DBLP:journals/corr/KingmaB14}{10}
\bibcite{Kriesel2007NeuralNetworks}{11}
\bibcite{Notter2018}{12}
\bibcite{Powell2007ADP}{13}
\@writefile{toc}{\contentsline {chapter}{Literaturverzeichnis}{65}{figure.A.4}}
\bibcite{ReddyE4877}{14}
\bibcite{DBLP:journals/corr/SchulmanLMJA15}{15}
\bibcite{Silver2015}{16}
\bibcite{SuttonBarto2018}{17}
\bibcite{Tsitsiklis2003}{18}
\bibcite{Wirth2015MeteorologicalPP}{19}
\bibcite{Zuern2017}{20}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.79976pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{28.8524pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{37.9912pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{25.91077pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{28.8524pt}
