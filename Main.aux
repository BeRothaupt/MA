\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Erkl\IeC {\"a}rung}{iii}{chapter*.1}}
\@writefile{toc}{\contentsline {chapter}{Kurzfassung / Abstract}{v}{section*.2}}
\citation{Notter2018}
\citation{Zuern2017}
\@writefile{toc}{\contentsline {chapter}{List of Figures}{ix}{chapter*.4}}
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xi}{chapter*.5}}
\@writefile{toc}{\contentsline {chapter}{Nomenklatur}{xiii}{chapter*.7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Zuern2017}
\citation{Zuern2017}
\citation{Bellman1954}
\citation{Bellman1962}
\citation{Powell2007ADP}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter2}{{2}{5}{Reinforcement Learning}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Process}{5}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Bellman Equation}{5}{section.2.2}}
\citation{SuttonBarto2018}
\newlabel{eq:bellman_exp_eq_V_Q}{{\relax 2.4}{6}{The Bellman Equation}{equation.2.2.4}{}}
\newlabel{eq:bellman_exp_eq_V_determinisic}{{\relax 2.5}{6}{The Bellman Equation}{equation.2.2.5}{}}
\newlabel{eq:bellman_optimality_equation_v_with_q}{{\relax 2.6}{6}{The Bellman Equation}{equation.2.2.6}{}}
\newlabel{eq:bellman_optimality_equation_stochastic}{{\relax 2.10}{6}{The Bellman Equation}{equation.2.2.10}{}}
\newlabel{eq:bellman_optimality_equation_deterministic}{{\relax 2.11}{6}{The Bellman Equation}{equation.2.2.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The Agent - Environment System}{6}{section.2.3}}
\citation{Notter2018}
\citation{Notter2018}
\citation{SuttonBarto2018}
\citation{SuttonBarto2018}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Model Based and Model Free Learning}{7}{section.2.4}}
\newlabel{sec:modelbasedmodelfree}{{2.4}{7}{Model Based and Model Free Learning}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces The Agent-Environment-System \cite  {Notter2018}}}{8}{figure.2.1}}
\newlabel{fig:agent_env_system}{{\relax 2.1}{8}{The Agent-Environment-System \cite {Notter2018}}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Agent}{8}{section.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Return}{9}{section.2.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Policy}{10}{section.2.7}}
\newlabel{sec:policy}{{2.7}{10}{Policy}{section.2.7}{}}
\newlabel{eq:transprobpi}{{\relax 2.17}{10}{Policy}{equation.2.7.17}{}}
\newlabel{eq:rewardpi}{{\relax 2.18}{10}{Policy}{equation.2.7.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Value Functions}{10}{section.2.8}}
\newlabel{sec:value-function}{{2.8}{10}{Value Functions}{section.2.8}{}}
\newlabel{eq:state_value_fun}{{\relax 2.19}{11}{Value Functions}{equation.2.8.19}{}}
\newlabel{eq:action_value_fun}{{\relax 2.20}{11}{Value Functions}{equation.2.8.20}{}}
\newlabel{eq:state_value_function_with_q}{{\relax 2.21}{11}{Value Functions}{equation.2.8.21}{}}
\newlabel{eq:action_value_function_with_v}{{\relax 2.22}{11}{Value Functions}{equation.2.8.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Solving an MDP}{11}{section.2.9}}
\citation{Zuern2017}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Dynamic Programming}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter3}{{3}{13}{Dynamic Programming}{chapter.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces Classification of Machine Learning algorithms}}{13}{figure.3.1}}
\newlabel{fig:classification_ml}{{\relax 3.1}{13}{Classification of Machine Learning algorithms}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces Figure 3.1 von Markus}}{14}{figure.3.2}}
\newlabel{fig:RLmethods}{{\relax 3.2}{14}{Figure 3.1 von Markus}{figure.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Approximate Dynamic Programming}{15}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Reward Function}{15}{section.3.2}}
\newlabel{sec:reward-function}{{3.2}{15}{Reward Function}{section.3.2}{}}
\newlabel{eq:reward_function}{{\relax 3.1}{15}{Reward Function}{equation.3.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Principle of Optimality}{15}{section.3.3}}
\newlabel{sec:optimality}{{3.3}{15}{The Principle of Optimality}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Types of Dynamic Programming Algorithms}{16}{section.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Policy Evaluation}{16}{subsection.3.4.1}}
\newlabel{subsection:policy_evaluation}{{3.4.1}{16}{Policy Evaluation}{subsection.3.4.1}{}}
\newlabel{eq:bellman_exp}{{\relax 3.4}{16}{Policy Evaluation}{equation.3.4.4}{}}
\newlabel{eq:bellman_exp_discrete_policy}{{\relax 3.5}{16}{Policy Evaluation}{equation.3.4.5}{}}
\newlabel{eq:bellman_exp_update}{{\relax 3.6}{16}{Policy Evaluation}{equation.3.4.6}{}}
\newlabel{eq:bellman_exp_update_bootstrapped}{{\relax 3.8}{16}{Policy Evaluation}{equation.3.4.8}{}}
\newlabel{eq:bellman_exp_update_discrete}{{\relax 3.9}{16}{Policy Evaluation}{equation.3.4.9}{}}
\newlabel{eq:pe_stopping_criterion}{{\relax 3.10}{17}{Policy Evaluation}{equation.3.4.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Iterative policy evaluation}}{17}{algorithm.1}}
\newlabel{algo:pe}{{1}{17}{Policy Evaluation}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Policy Iteration}{17}{subsection.3.4.2}}
\newlabel{sec:PI}{{3.4.2}{17}{Policy Iteration}{subsection.3.4.2}{}}
\citation{Silver2015}
\newlabel{eq:pi_scheme}{{3.4.2}{18}{Policy Iteration}{equation.3.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces The policy iteration algorithm}}{19}{figure.3.3}}
\newlabel{fig:PI_triangle}{{\relax 3.3}{19}{The policy iteration algorithm}{figure.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Value Iteration}{19}{subsection.3.4.3}}
\newlabel{sec:VI}{{3.4.3}{19}{Value Iteration}{subsection.3.4.3}{}}
\newlabel{eq:value_iteration_update}{{\relax 3.15}{19}{Value Iteration}{equation.3.4.15}{}}
\citation{Silver2015}
\newlabel{eq:vi_scheme}{{3.4.3}{20}{Value Iteration}{equation.3.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}The Contraction Mapping Theorem}{20}{section.3.5}}
\newlabel{sec:contraction_mappings}{{3.5}{20}{The Contraction Mapping Theorem}{section.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Discounted MDPs}{20}{section*.12}}
\newlabel{eq:bellman_operator_discounted}{{\relax 3.17}{20}{Discounted MDPs}{equation.3.5.17}{}}
\newlabel{eq:4.15}{{\relax 3.20}{21}{Discounted MDPs}{equation.3.5.20}{}}
\newlabel{eq:4.16}{{\relax 3.21}{21}{Discounted MDPs}{equation.3.5.21}{}}
\newlabel{eq:4.17}{{\relax 3.22}{21}{Discounted MDPs}{equation.3.5.22}{}}
\newlabel{eq:4.18}{{\relax 3.23}{21}{Discounted MDPs}{equation.3.5.23}{}}
\newlabel{eq:4.19}{{\relax 3.24}{21}{Discounted MDPs}{equation.3.5.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Undiscounted MDPs}{21}{section*.13}}
\newlabel{eq:bellman_undiscounted}{{\relax 3.25}{21}{Undiscounted MDPs}{equation.3.5.25}{}}
\newlabel{eq:n-step-contraction}{{\relax 3.26}{21}{Undiscounted MDPs}{equation.3.5.26}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Generalized policy iteration}}{22}{algorithm.2}}
\newlabel{algo:gpi}{{2}{22}{Policy Iteration}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Value iteration}}{23}{algorithm.3}}
\newlabel{algo:vi}{{3}{23}{Value Iteration}{algorithm.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Function Approximation}{25}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter4}{{4}{25}{Function Approximation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Tables}{25}{section.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Nearest Neighbor}{25}{section*.14}}
\citation{Kriesel2007NeuralNetworks}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Linear Interpolation}{26}{section*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Artificial Neural Networks}{26}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.1}{\ignorespaces A neuron in an artificial neural network}}{26}{figure.4.1}}
\newlabel{tikz:neuron}{{\relax 4.1}{26}{A neuron in an artificial neural network}{figure.4.1}{}}
\citation{Zuern2017}
\citation{Zuern2017}
\newlabel{eq:theta}{{\relax 4.3}{27}{Artificial Neural Networks}{equation.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.2}{\ignorespaces Activation functions \cite  {Zuern2017}}}{28}{figure.4.2}}
\newlabel{fig:activation_functions}{{\relax 4.2}{28}{Activation functions \cite {Zuern2017}}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.3}{\ignorespaces The MLP used for the policy}}{29}{figure.4.3}}
\newlabel{tikz:MLP}{{\relax 4.3}{29}{The MLP used for the policy}{figure.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Supervised Learning}{30}{section.4.3}}
\newlabel{lossfun}{{\relax 4.6}{30}{Supervised Learning}{equation.4.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Optimization Techniques}{30}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Gradient Descent}{31}{subsection.4.4.1}}
\newlabel{sec:grad_desc}{{4.4.1}{31}{Gradient Descent}{subsection.4.4.1}{}}
\newlabel{eq:gd_update}{{\relax 4.7}{31}{Gradient Descent}{equation.4.4.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Stochastic Gradient Descent}{31}{subsection.4.4.2}}
\newlabel{sec:sgd}{{4.4.2}{31}{Stochastic Gradient Descent}{subsection.4.4.2}{}}
\citation{DBLP:journals/corr/KingmaB14}
\newlabel{sgd_gradient}{{\relax 4.10}{32}{Stochastic Gradient Descent}{equation.4.4.10}{}}
\newlabel{sgd_update}{{\relax 4.11}{32}{Stochastic Gradient Descent}{equation.4.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}The Adam Algorithm}{32}{subsection.4.4.3}}
\newlabel{sec:adam}{{4.4.3}{32}{The Adam Algorithm}{subsection.4.4.3}{}}
\newlabel{eq:adam_update}{{\relax 4.12}{32}{The Adam Algorithm}{equation.4.4.12}{}}
\newlabel{eq:adam_hat_m}{{\relax 4.13}{32}{The Adam Algorithm}{equation.4.4.13}{}}
\newlabel{eq:adam_hat_v}{{\relax 4.14}{32}{The Adam Algorithm}{equation.4.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Overfitting}{33}{section.4.5}}
\newlabel{eq:overfitting_fitting}{{\relax 4.16}{33}{Overfitting}{equation.4.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.4}{\ignorespaces Overfitting in case of a high order polynomial and noisy data}}{34}{figure.4.4}}
\newlabel{tikz:overfitting}{{\relax 4.4}{34}{Overfitting in case of a high order polynomial and noisy data}{figure.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.5}{\ignorespaces Stop criterion to avoid overfitting}}{35}{figure.4.5}}
\newlabel{tikz:stopoverfitting}{{\relax 4.5}{35}{Stop criterion to avoid overfitting}{figure.4.5}{}}
\citation{Fichter2009}
\citation{Fichter2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Trajectory Optimization with Policy Iteration}{37}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter5}{{5}{37}{Trajectory Optimization with Policy Iteration}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}2D Environment}{37}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Policy Representation}{37}{subsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Equations of Motion}{37}{subsection.5.1.2}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 5.1}{\ignorespaces Grid parameters for trajectory optimization}}{38}{table.5.1}}
\newlabel{tab:grids}{{\relax 5.1}{38}{Grid parameters for trajectory optimization}{table.5.1}{}}
\newlabel{eq:dotgamma}{{\relax 5.4}{38}{Equations of Motion}{equation.5.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Discretization of the State- and Action Space}{38}{subsection.5.1.3}}
\newlabel{sec:disc2d}{{5.1.3}{38}{Discretization of the State- and Action Space}{subsection.5.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 5.1}{\ignorespaces The discretized state space in all 2d scenarios. For $x_4$ and $z_3$, the grid for the speed vector is drawn.}}{39}{figure.5.1}}
\newlabel{tikz:2d_state_space_discretized}{{\relax 5.1}{39}{The discretized state space in all 2d scenarios. For $x_4$ and $z_3$, the grid for the speed vector is drawn}{figure.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}3D Environment}{40}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Discretization of the state and action space}{40}{subsection.5.2.1}}
\newlabel{sec:disc3d}{{5.2.1}{40}{Discretization of the state and action space}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Results}{41}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chapter6}{{6}{41}{Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}2D Policy Iteration}{41}{section.6.1}}
\newlabel{sec:results2d}{{6.1}{41}{2D Policy Iteration}{section.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 6.1}{\ignorespaces Comparison of flight- and computation-time for OC, GPI, OPI and VI in scenario 1}}{42}{table.6.1}}
\newlabel{tab:2d_flight_data_500m}{{\relax 6.1}{42}{Comparison of flight- and computation-time for OC, GPI, OPI and VI in scenario 1}{table.6.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 6.2}{\ignorespaces Comparison of flight- and computation-time for OC, OPI and VI in scenario 2}}{42}{table.6.2}}
\newlabel{tab:2d_flight_data_1000m}{{\relax 6.2}{42}{Comparison of flight- and computation-time for OC, OPI and VI in scenario 2}{table.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.1}{\ignorespaces Results of generalized policy iteration (blue) and optimal control (red) in scenario 1}}{43}{figure.6.1}}
\newlabel{tikz:gpi500m}{{\relax 6.1}{43}{Results of generalized policy iteration (blue) and optimal control (red) in scenario 1}{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.2}{\ignorespaces Results of optimistic policy iteration (blue) and optimal control (red) in scenario 1}}{44}{figure.6.2}}
\newlabel{tikz:opi500m}{{\relax 6.2}{44}{Results of optimistic policy iteration (blue) and optimal control (red) in scenario 1}{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.3}{\ignorespaces Results of value iteration (blue) and optimal control (red) in scenario 1}}{45}{figure.6.3}}
\newlabel{tikz:vi500m}{{\relax 6.3}{45}{Results of value iteration (blue) and optimal control (red) in scenario 1}{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.4}{\ignorespaces Results of value iteration (blue) and optimal control (red)}}{46}{figure.6.4}}
\newlabel{tikz:vi1000m}{{\relax 6.4}{46}{Results of value iteration (blue) and optimal control (red)}{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.5}{\ignorespaces Average return per iteration with and without policy initialization}}{47}{figure.6.5}}
\newlabel{tikz:trpoglorotvi1000mAvg}{{\relax 6.5}{47}{Average return per iteration with and without policy initialization}{figure.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 6.6}{\ignorespaces Maximum return per iteration with and without policy initialization}}{48}{figure.6.6}}
\newlabel{tikz:trpoglorotvi1000mMax}{{\relax 6.6}{48}{Maximum return per iteration with and without policy initialization}{figure.6.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Discussion}{49}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix A: Glider Parameters and Scenario Data}{51}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix_A}{{A}{51}{Appendix A: Glider Parameters and Scenario Data}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax A.1}{\ignorespaces glider data}}{51}{table.A.1}}
\newlabel{tab:glider_data}{{\relax A.1}{51}{glider data}{table.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax A.2}{\ignorespaces scenario data}}{51}{table.A.2}}
\newlabel{tab:scenario_data}{{\relax A.2}{51}{scenario data}{table.A.2}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Appendix B: Computer Configuration}{53}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix_B}{{B}{53}{Appendix B: Computer Configuration}{appendix.B}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax B.1}{\ignorespaces computer specifications}}{53}{table.B.1}}
\newlabel{tab:pc_specs}{{\relax B.1}{53}{computer specifications}{table.B.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Appendix C: Table Representation of the Value Function and Policy}{55}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix_C}{{C}{55}{Appendix C: Table Representation of the Value Function and Policy}{appendix.C}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {D}Appendix D: Optimistic Policy Iteration}{57}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{appendix_D}{{D}{57}{Appendix D: Optimistic Policy Iteration}{appendix.D}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Optimistic Policy Iteration}}{57}{algorithm.4}}
\newlabel{algo:opi}{{4}{57}{Appendix D: Optimistic Policy Iteration}{algorithm.4}{}}
\bibstyle{abbrvdin}
\bibdata{./src/bib/bibfile}
\bibcite{Bellman1954}{1}
\bibcite{Bellman1962}{2}
\bibcite{Fichter2009}{3}
\bibcite{DBLP:journals/corr/KingmaB14}{4}
\bibcite{Kriesel2007NeuralNetworks}{5}
\bibcite{Notter2018}{6}
\bibcite{Powell2007ADP}{7}
\bibcite{Silver2015}{8}
\bibcite{SuttonBarto2018}{9}
\bibcite{Zuern2017}{10}
\@writefile{toc}{\contentsline {chapter}{Literaturverzeichnis}{59}{algorithm.4}}
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.79976pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{22.97746pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{32.11626pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{25.91077pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{22.97746pt}
