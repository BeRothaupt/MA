\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces The Agent-Environment-System \cite {Notter2018}}}{8}{figure.2.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces Classification of Machine Learning algorithms}}{13}{figure.3.1}
\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces Figure 3.1 von Markus}}{14}{figure.3.2}
\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces The policy iteration algorithm}}{19}{figure.3.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 4.1}{\ignorespaces A neuron in an artificial neural network}}{26}{figure.4.1}
\contentsline {figure}{\numberline {\relax 4.2}{\ignorespaces Activation functions \cite {Zuern2017}}}{28}{figure.4.2}
\contentsline {figure}{\numberline {\relax 4.3}{\ignorespaces The MLP used for the policy}}{29}{figure.4.3}
\contentsline {figure}{\numberline {\relax 4.4}{\ignorespaces Overfitting in case of a high order polynomial and noisy data}}{33}{figure.4.4}
\contentsline {figure}{\numberline {\relax 4.5}{\ignorespaces Stop criterion to avoid overfitting}}{35}{figure.4.5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 5.1}{\ignorespaces The discretized state space in all 2d scenarios. For $x_4$ and $z_3$, the grid for the speed vector is drawn.}}{39}{figure.5.1}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {\relax 6.1}{\ignorespaces Results of generalized policy iteration (blue) and optimal control (red)}}{43}{figure.6.1}
\contentsline {figure}{\numberline {\relax 6.2}{\ignorespaces Results of optimistic policy iteration (blue) and optimal control (red)}}{44}{figure.6.2}
\contentsline {figure}{\numberline {\relax 6.3}{\ignorespaces Results of value iteration (blue) and optimal control (red)}}{45}{figure.6.3}
\contentsline {figure}{\numberline {\relax 6.4}{\ignorespaces Results of value iteration (blue) and optimal control (red)}}{46}{figure.6.4}
\contentsline {figure}{\numberline {\relax 6.5}{\ignorespaces Average return per iteration with and without policy initialization}}{47}{figure.6.5}
\contentsline {figure}{\numberline {\relax 6.6}{\ignorespaces Maximum return per iteration with and without policy initialization}}{48}{figure.6.6}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
