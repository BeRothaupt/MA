\BOOKMARK [0][-]{chapter*.1}{Erkl\344rung}{}% 1
\BOOKMARK [0][-]{section*.2}{Kurzfassung / Abstract}{}% 2
\BOOKMARK [0][-]{chapter*.4}{List of Figures}{}% 3
\BOOKMARK [0][-]{chapter*.5}{List of Tables}{}% 4
\BOOKMARK [0][-]{chapter*.7}{Nomenklatur}{}% 5
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 6
\BOOKMARK [0][-]{chapter.2}{Solution Methods for Markov Decision Processes}{}% 7
\BOOKMARK [1][-]{section.2.1}{Markov Decision Process}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.2}{The Principle of Optimality}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.3}{The Agent - Environment System}{chapter.2}% 10
\BOOKMARK [1][-]{section.2.4}{Model Based and Model Free Learning}{chapter.2}% 11
\BOOKMARK [1][-]{section.2.5}{Agent}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.6}{Return}{chapter.2}% 13
\BOOKMARK [1][-]{section.2.7}{Policy}{chapter.2}% 14
\BOOKMARK [1][-]{section.2.8}{Value Functions}{chapter.2}% 15
\BOOKMARK [1][-]{section.2.9}{The Bellman Equation}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.10}{Solving an MDP}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.11}{Dynamic Programming}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.11.1}{Approximate Dynamic Programming}{section.2.11}% 19
\BOOKMARK [2][-]{subsection.2.11.2}{Types of Dynamic Programming Algorithms}{section.2.11}% 20
\BOOKMARK [2][-]{subsection.2.11.3}{The Contraction Mapping Theorem}{section.2.11}% 21
\BOOKMARK [1][-]{section.2.12}{Exploiting the specific Problem Structure}{chapter.2}% 22
\BOOKMARK [0][-]{chapter.3}{Function Approximation}{}% 23
\BOOKMARK [1][-]{section.3.1}{Tables}{chapter.3}% 24
\BOOKMARK [1][-]{section.3.2}{Artificial Neural Networks}{chapter.3}% 25
\BOOKMARK [1][-]{section.3.3}{Supervised Learning}{chapter.3}% 26
\BOOKMARK [1][-]{section.3.4}{Optimization Techniques}{chapter.3}% 27
\BOOKMARK [2][-]{subsection.3.4.1}{Gradient Descent}{section.3.4}% 28
\BOOKMARK [2][-]{subsection.3.4.2}{Stochastic Gradient Descent}{section.3.4}% 29
\BOOKMARK [2][-]{subsection.3.4.3}{The Adam Algorithm}{section.3.4}% 30
\BOOKMARK [1][-]{section.3.5}{Overfitting}{chapter.3}% 31
\BOOKMARK [0][-]{chapter.4}{Trajectory Optimization with Dynamic Programming}{}% 32
\BOOKMARK [1][-]{section.4.1}{2D Environment}{chapter.4}% 33
\BOOKMARK [2][-]{subsection.4.1.1}{Policy Representation}{section.4.1}% 34
\BOOKMARK [2][-]{subsection.4.1.2}{Equations of Motion}{section.4.1}% 35
\BOOKMARK [2][-]{subsection.4.1.3}{Reward Function}{section.4.1}% 36
\BOOKMARK [2][-]{subsection.4.1.4}{Discretization of the State- and Action Space}{section.4.1}% 37
\BOOKMARK [1][-]{section.4.2}{3D Environment}{chapter.4}% 38
\BOOKMARK [2][-]{subsection.4.2.1}{Discretization of the state and action space}{section.4.2}% 39
\BOOKMARK [0][-]{chapter.5}{Results}{}% 40
\BOOKMARK [1][-]{section.5.1}{2D Dynamic Programming}{chapter.5}% 41
\BOOKMARK [2][-]{subsection.5.1.1}{Optimal Control Benchmark}{section.5.1}% 42
\BOOKMARK [2][-]{subsection.5.1.2}{Policy Initialization for TRPO}{section.5.1}% 43
\BOOKMARK [0][-]{chapter.6}{Discussion}{}% 44
\BOOKMARK [0][-]{appendix.A}{Appendix}{}% 45
\BOOKMARK [1][-]{section.A.1}{Glider Parameters}{appendix.A}% 46
\BOOKMARK [1][-]{section.A.2}{Computer Configuration and Implementation}{appendix.A}% 47
\BOOKMARK [2][-]{subsection.A.2.1}{Computer Configuration}{section.A.2}% 48
\BOOKMARK [2][-]{subsection.A.2.2}{Computational Subtleties of PI and VI}{section.A.2}% 49
\BOOKMARK [1][-]{section.A.3}{Table Representation of the Value Function and Policy}{appendix.A}% 50
\BOOKMARK [1][-]{section.A.4}{Optimistic Policy Iteration}{appendix.A}% 51
\BOOKMARK [0][-]{algorithm.4}{Literaturverzeichnis}{}% 52
