\BOOKMARK [0][-]{chapter*.1}{Erkl\344rung}{}% 1
\BOOKMARK [0][-]{section*.2}{Kurzfassung / Abstract}{}% 2
\BOOKMARK [0][-]{chapter*.4}{List of Figures}{}% 3
\BOOKMARK [0][-]{chapter*.5}{List of Tables}{}% 4
\BOOKMARK [0][-]{chapter*.7}{Nomenklatur}{}% 5
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 6
\BOOKMARK [0][-]{chapter.2}{Solution Methods for Markov Decision Processes}{}% 7
\BOOKMARK [1][-]{section.2.1}{Markov Decision Process}{chapter.2}% 8
\BOOKMARK [1][-]{section.2.2}{The Bellman Equation}{chapter.2}% 9
\BOOKMARK [1][-]{section.2.3}{The Agent - Environment System}{chapter.2}% 10
\BOOKMARK [1][-]{section.2.4}{Model Based and Model Free Learning}{chapter.2}% 11
\BOOKMARK [1][-]{section.2.5}{Agent}{chapter.2}% 12
\BOOKMARK [1][-]{section.2.6}{Return}{chapter.2}% 13
\BOOKMARK [1][-]{section.2.7}{Policy}{chapter.2}% 14
\BOOKMARK [1][-]{section.2.8}{Value Functions}{chapter.2}% 15
\BOOKMARK [1][-]{section.2.9}{Solving an MDP}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.10}{The Principle of Optimality}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.11}{Dynamic Programming}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.11.1}{Approximate Dynamic Programming}{section.2.11}% 19
\BOOKMARK [2][-]{subsection.2.11.2}{Reward Function}{section.2.11}% 20
\BOOKMARK [2][-]{subsection.2.11.3}{Types of Dynamic Programming Algorithms}{section.2.11}% 21
\BOOKMARK [2][-]{subsection.2.11.4}{The Contraction Mapping Theorem}{section.2.11}% 22
\BOOKMARK [0][-]{chapter.3}{Function Approximation}{}% 23
\BOOKMARK [1][-]{section.3.1}{Tables}{chapter.3}% 24
\BOOKMARK [1][-]{section.3.2}{Artificial Neural Networks}{chapter.3}% 25
\BOOKMARK [1][-]{section.3.3}{Supervised Learning}{chapter.3}% 26
\BOOKMARK [1][-]{section.3.4}{Optimization Techniques}{chapter.3}% 27
\BOOKMARK [2][-]{subsection.3.4.1}{Gradient Descent}{section.3.4}% 28
\BOOKMARK [2][-]{subsection.3.4.2}{Stochastic Gradient Descent}{section.3.4}% 29
\BOOKMARK [2][-]{subsection.3.4.3}{The Adam Algorithm}{section.3.4}% 30
\BOOKMARK [1][-]{section.3.5}{Overfitting}{chapter.3}% 31
\BOOKMARK [0][-]{chapter.4}{Trajectory Optimization with Dynamic Programming}{}% 32
\BOOKMARK [1][-]{section.4.1}{2D Environment}{chapter.4}% 33
\BOOKMARK [2][-]{subsection.4.1.1}{Policy Representation}{section.4.1}% 34
\BOOKMARK [2][-]{subsection.4.1.2}{Equations of Motion}{section.4.1}% 35
\BOOKMARK [2][-]{subsection.4.1.3}{Discretization of the State- and Action Space}{section.4.1}% 36
\BOOKMARK [1][-]{section.4.2}{3D Environment}{chapter.4}% 37
\BOOKMARK [2][-]{subsection.4.2.1}{Discretization of the state and action space}{section.4.2}% 38
\BOOKMARK [0][-]{chapter.5}{Results}{}% 39
\BOOKMARK [1][-]{section.5.1}{2D Policy Iteration}{chapter.5}% 40
\BOOKMARK [0][-]{chapter.6}{Discussion}{}% 41
\BOOKMARK [0][-]{appendix.A}{Appendix A: Glider Parameters and Scenario Data}{}% 42
\BOOKMARK [0][-]{appendix.B}{Appendix B: Computer Configuration}{}% 43
\BOOKMARK [0][-]{appendix.C}{Appendix C: Table Representation of the Value Function and Policy}{}% 44
\BOOKMARK [0][-]{appendix.D}{Appendix D: Optimistic Policy Iteration}{}% 45
\BOOKMARK [0][-]{algorithm.4}{Literaturverzeichnis}{}% 46
