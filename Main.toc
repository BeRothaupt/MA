\babel@toc {english}{}
\contentsline {chapter}{Erkl\IeC {\"a}rung}{iii}{chapter*.1}
\contentsline {chapter}{Kurzfassung / Abstract}{v}{section*.2}
\contentsline {chapter}{List of Figures}{ix}{chapter*.4}
\contentsline {chapter}{List of Tables}{xi}{chapter*.5}
\contentsline {chapter}{Nomenklatur}{xiii}{chapter*.7}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {chapter}{\numberline {2}Solution Methods for Markov Decision Processes}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Process}{5}{section.2.1}
\contentsline {section}{\numberline {2.2}The Principle of Optimality}{5}{section.2.2}
\contentsline {section}{\numberline {2.3}The Agent - Environment System}{6}{section.2.3}
\contentsline {section}{\numberline {2.4}Model Based and Model Free Learning}{6}{section.2.4}
\contentsline {section}{\numberline {2.5}Agent}{8}{section.2.5}
\contentsline {section}{\numberline {2.6}Return}{8}{section.2.6}
\contentsline {section}{\numberline {2.7}Policy}{9}{section.2.7}
\contentsline {section}{\numberline {2.8}Value Functions}{10}{section.2.8}
\contentsline {section}{\numberline {2.9}The Bellman Equation}{10}{section.2.9}
\contentsline {section}{\numberline {2.10}Solving an MDP}{11}{section.2.10}
\contentsline {section}{\numberline {2.11}Dynamic Programming}{12}{section.2.11}
\contentsline {subsection}{\numberline {2.11.1}Approximate Dynamic Programming}{13}{subsection.2.11.1}
\contentsline {subsection}{\numberline {2.11.2}Types of Dynamic Programming Algorithms}{13}{subsection.2.11.2}
\contentsline {subsubsection}{\nonumberline Policy Evaluation}{13}{section*.12}
\contentsline {subsubsection}{\nonumberline Policy Iteration}{15}{section*.13}
\contentsline {subsubsection}{\nonumberline Value Iteration}{17}{section*.14}
\contentsline {subsection}{\numberline {2.11.3}The Contraction Mapping Theorem}{18}{subsection.2.11.3}
\contentsline {chapter}{\numberline {3}Function Approximation}{23}{chapter.3}
\contentsline {section}{\numberline {3.1}Tables}{23}{section.3.1}
\contentsline {subsubsection}{\nonumberline Nearest Neighbor}{23}{section*.17}
\contentsline {subsubsection}{\nonumberline Linear Interpolation}{24}{section*.18}
\contentsline {section}{\numberline {3.2}Artificial Neural Networks}{24}{section.3.2}
\contentsline {section}{\numberline {3.3}Supervised Learning}{28}{section.3.3}
\contentsline {section}{\numberline {3.4}Optimization Techniques}{28}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Gradient Descent}{29}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Stochastic Gradient Descent}{29}{subsection.3.4.2}
\contentsline {subsection}{\numberline {3.4.3}The Adam Algorithm}{30}{subsection.3.4.3}
\contentsline {section}{\numberline {3.5}Overfitting}{31}{section.3.5}
\contentsline {chapter}{\numberline {4}Trajectory Optimization with Dynamic Programming}{35}{chapter.4}
\contentsline {section}{\numberline {4.1}2D Environment}{35}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Policy Representation}{35}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Equations of Motion}{35}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Reward Function}{37}{subsection.4.1.3}
\contentsline {subsection}{\numberline {4.1.4}Discretization of the State- and Action Space}{37}{subsection.4.1.4}
\contentsline {section}{\numberline {4.2}3D Environment}{39}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Discretization of the state and action space}{39}{subsection.4.2.1}
\contentsline {chapter}{\numberline {5}Results}{41}{chapter.5}
\contentsline {section}{\numberline {5.1}2D Policy Iteration}{41}{section.5.1}
\contentsline {chapter}{\numberline {6}Discussion}{49}{chapter.6}
\contentsline {chapter}{\numberline {A}Appendix A: Glider Parameters and Scenario Data}{51}{appendix.A}
\contentsline {chapter}{\numberline {B}Appendix B: Computer Configuration}{53}{appendix.B}
\contentsline {chapter}{\numberline {C}Appendix C: Table Representation of the Value Function and Policy}{55}{appendix.C}
\contentsline {chapter}{\numberline {D}Appendix D: Optimistic Policy Iteration}{57}{appendix.D}
\contentsline {chapter}{Literaturverzeichnis}{59}{algorithm.4}
