\babel@toc {english}{}
\contentsline {chapter}{Erkl\IeC {\"a}rung}{iii}{chapter*.1}
\contentsline {chapter}{Abstract/Kurzfassung}{v}{chapter*.1}
\contentsline {chapter}{List of Figures}{ix}{chapter*.4}
\contentsline {chapter}{List of Tables}{xi}{chapter*.5}
\contentsline {chapter}{Nomenklatur}{xiii}{chapter*.7}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {chapter}{\numberline {2}Solution Methods for Markov Decision Processes}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Process}{5}{section.2.1}
\contentsline {section}{\numberline {2.2}The Principle of Optimality}{5}{section.2.2}
\contentsline {section}{\numberline {2.3}The Agent - Environment System}{6}{section.2.3}
\contentsline {section}{\numberline {2.4}Model Based and Model Free Learning}{6}{section.2.4}
\contentsline {section}{\numberline {2.5}Agent}{8}{section.2.5}
\contentsline {section}{\numberline {2.6}Return}{8}{section.2.6}
\contentsline {section}{\numberline {2.7}Policy}{9}{section.2.7}
\contentsline {section}{\numberline {2.8}Value Functions}{10}{section.2.8}
\contentsline {section}{\numberline {2.9}The Bellman Equation}{10}{section.2.9}
\contentsline {section}{\numberline {2.10}Solving an MDP}{11}{section.2.10}
\contentsline {section}{\numberline {2.11}Dynamic Programming}{12}{section.2.11}
\contentsline {subsection}{\numberline {2.11.1}The Curses of Dimensionality}{13}{subsection.2.11.1}
\contentsline {subsection}{\numberline {2.11.2}Approximate Dynamic Programming}{14}{subsection.2.11.2}
\contentsline {subsection}{\numberline {2.11.3}Types of Dynamic Programming Algorithms}{16}{subsection.2.11.3}
\contentsline {subsubsection}{\nonumberline Policy Evaluation}{16}{section*.12}
\contentsline {subsubsection}{\nonumberline Policy Iteration}{17}{section*.13}
\contentsline {subsubsection}{\nonumberline Value Iteration}{19}{section*.14}
\contentsline {subsection}{\numberline {2.11.4}The Contraction Mapping Theorem}{20}{subsection.2.11.4}
\contentsline {section}{\numberline {2.12}Exploiting the Problem Structure}{21}{section.2.12}
\contentsline {chapter}{\numberline {3}Function Approximation}{25}{chapter.3}
\contentsline {section}{\numberline {3.1}Tables}{25}{section.3.1}
\contentsline {subsubsection}{\nonumberline Nearest Neighbor}{25}{section*.17}
\contentsline {subsubsection}{\nonumberline Linear Interpolation}{26}{section*.18}
\contentsline {section}{\numberline {3.2}Artificial Neural Networks}{26}{section.3.2}
\contentsline {section}{\numberline {3.3}Supervised Learning}{30}{section.3.3}
\contentsline {section}{\numberline {3.4}Optimization Techniques}{31}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Gradient Descent}{31}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Stochastic Gradient Descent}{31}{subsection.3.4.2}
\contentsline {subsection}{\numberline {3.4.3}The Adam Algorithm}{32}{subsection.3.4.3}
\contentsline {section}{\numberline {3.5}Overfitting}{33}{section.3.5}
\contentsline {chapter}{\numberline {4}Trajectory Optimization with Dynamic Programming}{37}{chapter.4}
\contentsline {section}{\numberline {4.1}2D Environment}{37}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Policy Representation}{37}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Equations of Motion}{37}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Reward Function}{39}{subsection.4.1.3}
\contentsline {subsection}{\numberline {4.1.4}Discretization of the State- and Action Space}{39}{subsection.4.1.4}
\contentsline {chapter}{\numberline {5}Results}{43}{chapter.5}
\contentsline {section}{\numberline {5.1}2D Dynamic Programming}{43}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Optimal Control Benchmark}{43}{subsection.5.1.1}
\contentsline {subsubsection}{\nonumberline Scenario 1 - Coarse Discretization}{44}{section*.22}
\contentsline {subsubsection}{\nonumberline Scenario 1 - Fine Discretization}{45}{section*.23}
\contentsline {subsubsection}{\nonumberline Scenario 2}{47}{section*.24}
\contentsline {subsection}{\numberline {5.1.2}Policy Initialization for TRPO}{48}{subsection.5.1.2}
\contentsline {chapter}{\numberline {6}Discussion}{55}{chapter.6}
\contentsline {chapter}{\numberline {A}Appendix}{57}{appendix.A}
\contentsline {section}{\numberline {A.1}Glider Parameters}{57}{section.A.1}
\contentsline {section}{\numberline {A.2}Computer Configuration and Implementation}{58}{section.A.2}
\contentsline {subsection}{\numberline {A.2.1}Computer Configuration}{58}{subsection.A.2.1}
\contentsline {subsection}{\numberline {A.2.2}Computational Subtleties of PI and VI}{59}{subsection.A.2.2}
\contentsline {section}{\numberline {A.3}Illustration of a One Step Lookahead}{60}{section.A.3}
\contentsline {section}{\numberline {A.4}Optimistic Policy Iteration}{61}{section.A.4}
\contentsline {section}{\numberline {A.5}Table Representation of the Value Function and Policy}{62}{section.A.5}
\contentsline {section}{\numberline {A.6}Scenario 1: Additional Results}{63}{section.A.6}
\contentsline {chapter}{Literaturverzeichnis}{65}{figure.A.4}
