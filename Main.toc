\babel@toc {english}{}
\contentsline {chapter}{Erkl\IeC {\"a}rung}{iii}{chapter*.1}
\contentsline {chapter}{Kurzfassung / Abstract}{v}{section*.2}
\contentsline {chapter}{List of Figures}{ix}{chapter*.4}
\contentsline {chapter}{List of Tables}{xi}{chapter*.5}
\contentsline {chapter}{Nomenklatur}{xiii}{chapter*.7}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {chapter}{\numberline {2}Reinforcement Learning}{3}{chapter.2}
\contentsline {section}{\numberline {2.1}Markov Decision Process}{3}{section.2.1}
\contentsline {section}{\numberline {2.2}The Agent - Environment System}{3}{section.2.2}
\contentsline {section}{\numberline {2.3}Model Based and Model Free Learning}{4}{section.2.3}
\contentsline {section}{\numberline {2.4}Agent}{5}{section.2.4}
\contentsline {section}{\numberline {2.5}Return}{6}{section.2.5}
\contentsline {section}{\numberline {2.6}Policy}{6}{section.2.6}
\contentsline {section}{\numberline {2.7}Value Functions}{7}{section.2.7}
\contentsline {section}{\numberline {2.8}The Bellman Equation}{8}{section.2.8}
\contentsline {chapter}{\numberline {3}Dynamic Programming}{11}{chapter.3}
\contentsline {section}{\numberline {3.1}Approximate Dynamic Programming}{13}{section.3.1}
\contentsline {section}{\numberline {3.2}Reward Function}{13}{section.3.2}
\contentsline {section}{\numberline {3.3}The Principle of Optimality}{13}{section.3.3}
\contentsline {section}{\numberline {3.4}Types of Dynamic Programming Algorithms}{14}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Policy Evaluation}{14}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Policy Iteration}{15}{subsection.3.4.2}
\contentsline {subsection}{\numberline {3.4.3}Value Iteration}{17}{subsection.3.4.3}
\contentsline {section}{\numberline {3.5}The Contraction Mapping Theorem}{17}{section.3.5}
\contentsline {subsubsection}{\nonumberline Discounted MDPs}{18}{section*.12}
\contentsline {subsubsection}{\nonumberline Undiscounted MDPs}{18}{section*.13}
\contentsline {chapter}{\numberline {4}Function Approximation}{21}{chapter.4}
\contentsline {section}{\numberline {4.1}Tables}{21}{section.4.1}
\contentsline {subsubsection}{\nonumberline Nearest Neighbor}{21}{section*.14}
\contentsline {subsubsection}{\nonumberline Linear Interpolation}{22}{section*.15}
\contentsline {section}{\numberline {4.2}Artificial Neural Networks}{22}{section.4.2}
\contentsline {section}{\numberline {4.3}Supervised Learning}{26}{section.4.3}
\contentsline {section}{\numberline {4.4}Optimization Techniques}{27}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Gradient Descent}{27}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}Stochastic Gradient Descent}{28}{subsection.4.4.2}
\contentsline {subsection}{\numberline {4.4.3}The ADAM Algorithm}{28}{subsection.4.4.3}
\contentsline {section}{\numberline {4.5}Overfitting}{29}{section.4.5}
\contentsline {chapter}{\numberline {5}Trajectory Optimization with Policy Iteration}{31}{chapter.5}
\contentsline {section}{\numberline {5.1}2D Environment}{31}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Policy Representation}{31}{subsection.5.1.1}
\contentsline {subsection}{\numberline {5.1.2}Equations of Motion}{31}{subsection.5.1.2}
\contentsline {subsection}{\numberline {5.1.3}Discretization of the State- and Action Space}{33}{subsection.5.1.3}
\contentsline {section}{\numberline {5.2}3D Environment}{34}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Discretization of the state and action space}{34}{subsection.5.2.1}
\contentsline {chapter}{\numberline {6}Results}{35}{chapter.6}
\contentsline {section}{\numberline {6.1}2D Policy Iteration}{35}{section.6.1}
\contentsline {section}{\numberline {6.2}3D Scenarios}{37}{section.6.2}
\contentsline {chapter}{\numberline {7}Discussion}{39}{chapter.7}
\contentsline {chapter}{\numberline {A}Appendix A: Glider Parameters and Scenario Data}{41}{appendix.A}
\contentsline {chapter}{\numberline {B}Appendix B: Computer Configuration}{43}{appendix.B}
\contentsline {chapter}{\numberline {C}Appendix C: Table Representation of the Value Function and Policy}{45}{appendix.C}
\contentsline {chapter}{Literaturverzeichnis}{47}{appendix.C}
